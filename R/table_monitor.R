# WARNING - Generated by {fusen} from dev/station_pig_monitor.Rmd: do not edit by hand

#' Table feed intake monitor of pig performance test station
#' 
#' @param data A data frame or data table. This is the data to be processed. It should have specific columns depending on the station type.
#' @param house_width A character or numeric value specifying the width of the house number.If numeric, it will be converted to a character string. default house_width is 1.
#' @param days Days you can choose to monitor latest n days data, default days is 7.
#' @param save_path A character string specifying the path where the output PNG files will be saved.
#' 
#' @return A list of data.tables, each representing different aspects of station monitoring results. This list is also saved to an Excel file at the specified `save_path`.
#' @import data.table
#' @import openxlsx
#' @importFrom stats "quantile" "time"
#' @export
#' @examples
#' # Load CSV data
#' data <- data.table::fread("C:/Users/Dell/Desktop/test/monitor_test_data.csv")
#' # Monitor station and data
#' res <- table_monitor(data = data, save_path = "C:/Users/Dell/Desktop/test")
#' # Monitor the number of times 'na' appears in the last 7 days
#' res$responder_na
#' # Monitor the percentage of extreme weight records in the last 7 days
#' res$extreme_weight
#' # Monitor the visiting time and frequency of pigs in the last 7 days
#' res$feed_time_n
#' # Monitor the total feed intake over the last 7 days
#' res$all_feedintake
#' # Monitor the average feed intake over the last 7 days
#' res$mean_feedintake
#' # Monitor the average weight per pen over the last 7 days
#' res$house_weight
#' # Monitor visit time in each hour over the last 1 day.
#' res$visit_n
#' # Monitor feed intake time in each hour over the last 1 day.
#' res$feed_time
#' # Monitor feed intake in each hour over the last 1 day.
#' res$feed_intake
table_monitor <- function(data, house_width = "1", days = 7, save_path) {
  
  # Check parameters
  if (missing(data) || is.null(data)) stop("data cannot be NULL")
  if (is.data.frame(data)) data <- data.table::as.data.table(data.table::copy(data))
  if (is.null(house_width)) stop("house_width cannot be NULL")
  if (is.numeric(house_width)) house_width <- as.character(house_width)
  if (is.null(days)) stop("days cannot be NULL")
  if (is.character(days)) days <- as.integer(days)

  # Check save_path parameter
  if (missing(save_path) || is.null(save_path) || !is.character(save_path)) {
    stop("save_path must be a non-empty string ending with '.xlsx'")
  }

  # Processing data
  data <- unique(data)
  with_responder_na <- responder_na(data, days)
  with_extreme_time_n <- extreme_time_n(data, days)
  with_process_data <- process_data(data)
  with_low_feedintake <- low_feedintake(with_process_data, days)
  with_all_feedintake <- all_feedintake(with_process_data, days)
  with_number_location <- number_location(with_process_data, house_width, days)
  with_house_feedintake <- house_feedintake(with_process_data, house_width, days)
  with_merge_data <- merge_data(with_house_feedintake, with_number_location)
  with_house_weight <- house_weight(with_process_data, house_width, days)
  with_hour_stat <- hour_stat(data)

  all_monitor <- list(
    responder_na = with_responder_na,
    extreme_weight = with_extreme_time_n$extreme_ratio,
    feed_time_n = with_extreme_time_n$feed_time_n,
    all_feedintake = with_all_feedintake,
    mean_feedintake = with_merge_data,
    house_weight = with_house_weight,
    visit_n = with_hour_stat$visit_n,
    feed_time = with_hour_stat$feed_time,
    feed_intake = with_hour_stat$feed_intake
  )

  # Create a new xlsx file
  wb <- openxlsx::createWorkbook()

  # Loop through each sheet name in the all_monitor list
  for (sheet_name in names(all_monitor)) {
    data <- all_monitor[[sheet_name]]  # Extract the data for the current sheet

    # Add a worksheet to the workbook
    openxlsx::addWorksheet(wb, sheet_name)

    # Write the data to the worksheet
    openxlsx::writeData(wb, sheet = sheet_name, data, rowNames = FALSE, colNames = TRUE)

    # Define different rules for different data frames if needed
    if (sheet_name == "responder_na") {
      # Calculate the 25th, 50th, and 75th percentiles for the column sum_na
      q1 <- quantile(data$sum_na, 0.25, na.rm = TRUE)
      q2 <- quantile(data$sum_na, 0.5, na.rm = TRUE)
      q3 <- quantile(data$sum_na, 0.75, na.rm = TRUE)

      # Add conditional formatting based on the calculated percentiles
      openxlsx::conditionalFormatting(wb, sheet = sheet_name, cols = ncol(data), rows = 2:(nrow(data) + 1),
                            rule = c(q1, q2, q3),
                            type = "colorScale",
                            style = c("#63BE7B", "#FFEB84", "#F8696B"))
    } else if (sheet_name == "extreme_weight") {
      # Calculate the 25th, 50th, and 75th percentiles for the entire data frame
      data_values <- unlist(data[, 2:ncol(data)], use.names = FALSE)
      q1 <- quantile(data_values, 0.25, na.rm = TRUE)
      q2 <- quantile(data_values, 0.5, na.rm = TRUE)  # Median
      q3 <- quantile(data_values, 0.75, na.rm = TRUE)

      # Add conditional formatting based on the calculated percentiles
      openxlsx::conditionalFormatting(wb, sheet = sheet_name, cols = 2:(ncol(data) + 1), rows = 2:(nrow(data) + 1),
                            rule = c(q1, q2, q3),
                            type = "colorScale",
                            style = c("#63BE7B", "#FFEB84", "#F8696B"))
    } else if (sheet_name == "all_feedintake") {
      # Calculate the 25th, 50th, and 75th percentiles for the column sum_feed_intake
      q1 <- quantile(data$sum_feed_intake, 0.25, na.rm = TRUE)
      q2 <- quantile(data$sum_feed_intake, 0.5, na.rm = TRUE)
      q3 <- quantile(data$sum_feed_intake, 0.75, na.rm = TRUE)

      # Add conditional formatting based on the calculated percentiles
      openxlsx::conditionalFormatting(wb, sheet = sheet_name, cols = ncol(data), rows = 2:(nrow(data) + 1),
                            rule = c(q1, q2, q3),
                            type = "colorScale",
                            style = c("#F8696B", "#FFEB84", "#63BE7B"))
    } else if (sheet_name == "mean_feedintake") {
      # Select columns with names containing "mean"
      mean_cols <- grep("mean_feed", colnames(data), value = TRUE)
      n_cols <- length(mean_cols)

      # Calculate the 25th, 50th, and 75th percentiles for the entire data frame
      data_values <- unlist(data[, ..mean_cols], use.names = FALSE)
      q1 <- quantile(data_values, 0.25, na.rm = TRUE)
      q2 <- quantile(data_values, 0.5, na.rm = TRUE)  # Median
      q3 <- quantile(data_values, 0.75, na.rm = TRUE)

      # Add conditional formatting based on the calculated percentiles
      openxlsx::conditionalFormatting(wb, sheet = sheet_name, cols = (ncol(data) - n_cols + 1):ncol(data), rows = 2:(nrow(data) + 1),
                            rule = c(q1, q2, q3),
                            type = "colorScale",
                            style = c("#F8696B", "#FFEB84", "#63BE7B"))
    } else if (sheet_name == "house_weight") {
      # Calculate the 25th, 50th, and 75th percentiles for the entire data frame
      data_values <- unlist(data[, 2:ncol(data)], use.names = FALSE)
      q1 <- quantile(data_values, 0.25, na.rm = TRUE)
      q2 <- quantile(data_values, 0.5, na.rm = TRUE)  # Median
      q3 <- quantile(data_values, 0.75, na.rm = TRUE)

      # Add conditional formatting based on the calculated percentiles
      openxlsx::conditionalFormatting(wb, sheet = sheet_name, cols = 2:(ncol(data) + 1), rows = 2:(nrow(data) + 1),
                            rule = c(q1, q2, q3),
                            type = "colorScale",
                            style = c("#F8696B", "#FFEB84", "#63BE7B"))
    } else if (sheet_name %chin% c("visit_n", "feed_time", "feed_intake")) {
      # Calculate the 25th, 50th, and 75th percentiles for the entire data frame
      numeric_columns <- sapply(data, is.numeric)
      data_values <- unlist(data[, ..numeric_columns], use.names = FALSE)
      q1 <- quantile(data_values, 0.25, na.rm = TRUE)
      q2 <- quantile(data_values, 0.5, na.rm = TRUE)  # Median
      q3 <- quantile(data_values, 0.75, na.rm = TRUE)

      # Add conditional formatting based on the calculated percentiles
      openxlsx::conditionalFormatting(wb, sheet = sheet_name, cols = 2:(ncol(data) + 1), rows = 2:(nrow(data) + 1),
                            rule = c(q1, q2, q3),
                            type = "colorScale",
                            style = c("#F8696B", "#FFEB84", "#63BE7B"))
    } else {
      # Select columns with names containing "sum_n"
      sum_n_cols <- grep("sum_n", colnames(data), value = TRUE)
      n_cols <- length(sum_n_cols)

      # Calculate the 25th, 50th, and 75th percentiles for the entire data frame
      data_values <- unlist(data[, ..sum_n_cols], use.names = FALSE)
      q1 <- quantile(data_values, 0.25, na.rm = TRUE)
      q2 <- quantile(data_values, 0.5, na.rm = TRUE)  # Median
      q3 <- quantile(data_values, 0.75, na.rm = TRUE)

      # Add conditional formatting based on the calculated percentiles
      openxlsx::conditionalFormatting(wb, sheet = sheet_name, cols = (ncol(data) - n_cols + 1):ncol(data), rows = 2:(nrow(data) + 1),
                            rule = c(q1, q2, q3),
                            type = "colorScale",
                            style = c("#F8696B", "#FFEB84", "#63BE7B"))
    }

    # Create a bold style for column and row names
    header_style <- openxlsx::createStyle(textDecoration = "bold", halign = "center")

    # Apply the bold style to column names
    openxlsx::addStyle(wb, sheet = sheet_name, style = header_style, rows = 1, cols = 1:ncol(data), gridExpand = TRUE)

    # Create a center alignment style for cell contents
    center_style <- openxlsx::createStyle(halign = "center")

    # Apply the center alignment style to cell contents
    openxlsx::addStyle(wb, sheet = sheet_name, style = center_style, rows = 2:(nrow(data) + 1), cols = 1:ncol(data), gridExpand = TRUE)

    # Optionally, apply the bold style to row names
    # In this example, assume the first column contains row names
    openxlsx::addStyle(wb, sheet = sheet_name, style = header_style, rows = 1:(nrow(data) + 1), cols = 1, gridExpand = TRUE)

    # Auto adjust column widths
    openxlsx::setColWidths(wb, sheet = sheet_name, cols = 1:ncol(data), widths = "auto")
  }

  # Save the xlsx file
  openxlsx::saveWorkbook(wb, file.path(save_path, "table_monitor.xlsx"), overwrite = TRUE)

  return(all_monitor)
}

process_data <- function(data) {
  if (missing(data)) stop("Missing data frame or data table!")
  if (!is.data.frame(data) && !inherits(data, "data.table")) stop("Data is not a data frame or data table!")
  #if (!is.data.frame(data) && !class(data) == 'data.table')
  # Check for required columns
  required_columns <- c("animal_number", "lifenumber", "responder", "location", "visit_time", "duration", "state", "weight", "feed_intake")
  missing_columns <- setdiff(required_columns, names(data))
  if (length(missing_columns) > 0) stop(paste("Missing columns:", paste(missing_columns, collapse = ", ")))

  # Check types for some columns
  if (!is.numeric(data$animal_number) && !is.character(data$animal_number)) stop("'animal_number' must be numeric or character!")
  if (!is.logical(data$lifenumber) && !is.character(data$lifenumber)) stop("'lifenumber' must be logical or character!")
  if (!is.numeric(data$responder) && !is.character(data$responder)) stop("'responder' must be numeric or character!")
  if (!is.numeric(data$location)) stop("'location' must be numeric!")
  if (!is.character(data$visit_time) && !inherits(data$visit_time, "POSIXt")) stop("'visit_time' must be character or POSIXct!")
  if (!is.numeric(data$duration)) stop("'duration' must be numeric!")
  if (!is.numeric(data$state)) stop("'state' must be numeric!")
  if (!is.numeric(data$weight)) stop("'weight' must be numeric!")
  if (!is.numeric(data$feed_intake)) stop("'feed_intake' must be numeric!")

  # Check if the data is a data.frame, if yes, then make a deep copy of the data and convert it into data.table
  if (is.data.frame(data)) data <- data.table::as.data.table(data.table::copy(data))

  responder <- weight <- . <- location <- N <- n <- location_maxn <- visit_time <- seq_days <- seq_in_day <- seq_in_location <- feed_intake <- NULL

  # Filter out the data with NA in 'responder' column and remove duplicates
  data_temp <- unique(data)[!is.na(responder)]

  # Create a unique data.table for 'responder' and 'location'
  unique_dt <- unique(data_temp[, .(responder, location)])

  # Find duplicate 'responder's
  dup_responders <- unique_dt[, .N, by = .(responder)][N > 1]

  # Compute the number of records for each 'responder' and 'location'
  num_records <- unique(data_temp[, `:=`(n, .N), .(responder, location)][, .(responder, location, n)])

  # Set 'responder' as the key for join operations
  data.table::setkey(dup_responders, responder)
  data.table::setkey(num_records, responder)

  # Perform left join operation on 'num_records' and 'dup_responders'
  dup_records <- num_records[dup_responders]

  # Print duplicate 'responder's and their 'location's
  if(nrow(dup_records) > 0) {
    cat(crayon::red("\u2022 There are", length(unique(dup_records$responder)), "duplicated responders.\n"))
    print(dup_records)
  } else {
    cat(crayon::green("\u2022 There are no duplicate responders in different locations.\n"))
  }

  # Modify the 'location' in the unique data.table for duplicate 'responder's
  if(nrow(dup_responders) > 0) {
    # Compute the 'location' with maximum number of records for each 'responder'
    max_n_location <- num_records[, .(max_n = max(n), location_maxn = location[which.max(n)]), by = responder]

    # Remove duplicates in 'max_n_location' after modifying 'location'
    max_n_location <- unique(max_n_location)

    # Perform left join operation on 'data_temp' and 'max_n_location' and update 'location' to 'location_maxn'
    data_temp <- merge(data_temp, max_n_location, by = "responder", all.x = TRUE)[, location := location_maxn][, c("max_n", "location_maxn") := NULL]
  }

  # Preprocess data and compute sequence features
  # Check the class of visit_time
  if(is.character(data_temp$visit_time)) {
    # If visit_time is a character vector, replace "/" with "-"
    data_temp[, visit_time := gsub("/", "-", visit_time)]
    data_pre <- data_temp[, `:=`(c("date", "time"), data.table::IDateTime(visit_time))]
  } else {
    # If visit_time is not a character vector, assume it's a datetime object
    data_pre <- data_temp[, `:=`(c("date", "time"), data.table::IDateTime(visit_time))]
  }
  data_pre <- data_pre[data.table::CJ(date = tidyr::full_seq(date, 1)), on = .(date)  # Compute complete sequence of dates
  ][order(date), `:=`(seq_days, .GRP), by = date  # Compute sequence number of days
  ][order(visit_time), `:=`(seq_in_day, 1:.N), by = .(responder, date)  # Compute sequence number in day
  ][order(visit_time), `:=`(seq_in_location, 1:.N), by = .(location, date)  # Compute sequence number in location
  ][order(responder, visit_time)  # Order data by 'responder' and 'visit_time'
  ][, .(responder, location, date, feed_intake, weight)  # Keep only necessary columns #seq_in_location, seq_days, seq_in_day,, weight
  ][, `:=` (responder = as.character(responder),  # Convert 'responder' and 'location' to character type
            location = as.character(location),
            feed_intake = as.numeric(feed_intake),
            weight = as.numeric(weight))][!is.na(location)][]  # Convert 'weight' to numeric type
  return(data_pre)
}
low_feedintake <- function(data, days) {
  . <- feed_intake <- location <- responder <- sum_feedintake <- sum_feed_intake <- NULL
  # Filter rows, format the 'date', group by 'location', 'date', 'responder',
  # and calculate the sum of 'feed_intake' divided by 1000 where 'sum_feedintake' is less than 0.5
  processed_data <- data[date >= lubridate::today() - days][
    , date := format(date, "%y-%m-%d")][
      , .(sum_feedintake = sum(feed_intake) / 1000), by = .(location, date, responder)][
        sum_feedintake < 0.5
      ]

  # Convert to wide format with 'dcast'
  data_wide <- dcast(processed_data, location + responder ~ date, value.var = "sum_feedintake")

  # Sort by 'location' and calculate sum across numeric columns for each row
  numeric_cols <- setdiff(names(data_wide), c("location", "responder"))
  data_wide[, sum_feed_intake := round(rowSums(.SD, na.rm = TRUE), digits = 2), .SDcols = numeric_cols]
  #data_wide[, sum_feed_intake := round(Reduce(`+`, lapply(.SD, function(x) replace(x, is.na(x), 0))), 3), .SDcols = numeric_cols]

  # Re-sort based on 'sum_feed_intake'
  setorder(data_wide, sum_feed_intake)

  return(data_wide)
}
all_feedintake <- function(data, days) {
  . <- feed_intake <- location <- location_maxn <- sum_feedintake <- sum_feed_intake <- NULL
  # Step 1: Group by location and date, calculate the sum of feed intake divided by 1000
  # Filter the results for dates within the last week from November 1, 2023, format the date, and exclude rows with NA location
  res1 <- data[, .(sum_feedintake = sum(feed_intake) / 1000), by = .(location, date)
  ][date >= lubridate::today() - days
  ][, date := format(date, "%y-%m-%d")
  ]

  # Step 2: Transform the data into wide format using dcast
  res1_wide <- dcast(res1, location ~ date, value.var = "sum_feedintake")

  # Step 3: Identify numeric columns, excluding 'location' and potentially 'responder' columns
  numeric_cols <- setdiff(names(res1_wide), c("location", "responder"))

  # Step 4: Calculate the sum of all numeric columns for each row, ignoring NA values, and round the result to two decimal places
  res1_wide[, sum_feed_intake := round(rowSums(.SD, na.rm = TRUE), digits = 2), .SDcols = numeric_cols]

  # Return the processed data table
  return(res1_wide)
}
number_location <- function(data, house_width, days) {
  house <- location <- . <- responder <- NULL
  data[, house := substr(location, 1, house_width) # Step 1: Create 'house' column from the first character of 'location'
  ][date >= lubridate::today() - days # Step 2: Filter rows where 'date' is within the last 7 days from 2023-11-01
  ][, date := format(date, "%y-%m-%d") # Step 3: Format 'date' as 'year-month-day'
  ][, .(house, date, responder) # Step 4: Select the 'house', 'date', and 'responder' columns
  ][, unique(.SD, by = c("house", "date", "responder")) # Step 5: Remove duplicate rows based on 'house', 'date', 'responder'
  ][, .(n = .N), by = .(house, date) # Step 6: Group by 'house' and 'date', then count the number of rows in each group
  ][, dcast(.SD, date ~ house, value.var = "n")] # Step 7: Reshape the data into a wide format with 'date' as rows, 'house' as columns, and 'n' as values
}
house_feedintake <- function(data, house_width, days) {
  house <- location <- sum_feed_intake <- feed_intake <- . <- NULL
  temp <- data[, house := substr(location, 1, house_width)  # Step 1: Add a new column 'house' derived from the first character of 'location'.
  ][date >= lubridate::today() - days  # Step 2: Filter the data to include only the rows with 'date' within the last 7 days from 2023-11-01.
  ][, date := format(date, "%y-%m-%d")  # Step 3: Reformat 'date' to a 'year-month-day' string format.
  ][, sum_feed_intake := sum(feed_intake)/1000, by = .(house, date)  # Step 4: Sum 'feed_intake' divided by 1000 for each 'house' and 'date' group.
  ][, .(date, house, sum_feed_intake)  # Step 5: Select the 'date', 'house', and 'sum_feed_intake' columns for the output.
  ][, unique(.SD, by = c("house", "date"))  # Step 6: Remove duplicate rows based on 'house' and 'date'.
  ][, dcast(.SD, date ~ house, value.var = "sum_feed_intake")]  # Step 7: Reshape the data into a wide format with 'date' as rows, 'house' as columns, and summed feed intake as values.

  # After reshaping the data, calculate the total feed intake across all houses for each date.
  numeric_cols <- setdiff(names(temp), c("date"))  # Step 8: Identify numeric columns, excluding 'date'.
  temp[, sum_feed_intake := round(rowSums(.SD, na.rm = TRUE), digits = 2), .SDcols = numeric_cols]  # Step 9: Sum up the values across the numeric columns for each row, ignoring NAs, and round the result to two decimal places.

  return(temp)  # Step 10: Return the processed and reshaped data table.
}
merge_data <- function(feedintake, n) {
  . <- NULL
  # Merge hh2 and hh by "date", including all rows and adding suffixes to overlapping column names
  res3 <- merge(feedintake, n, by = "date", all = TRUE, suffixes = c("_feed", "_n"))

  # Extract column names exclusive to hh2 (excluding "date" and "sum_feed_intake")
  cols_feed <- names(feedintake)[!names(n) %in% c("date", "sum_feed_intake")]

  # Extract column names exclusive to hh (excluding "date")
  cols_n <- names(n)[!names(n) %in% c("date")]

  # Identify common columns between hh2 and hh for calculating mean feed intake
  intersect_house <- intersect(cols_feed, cols_n)

  # Identify columns unique to hh2 and hh, representing houses without corresponding data in the other table
  #set_diff_house_feed <- setdiff(cols_feed, cols_n) # Houses with feed data but no count data
  #set_diff_house_n <- setdiff(cols_n, cols_feed) # Houses with count data but no feed data

  # Append suffixes to column names for the merged table
  cols_feed_name <- paste0(intersect_house, "_feed")
  cols_n_name <- paste0(intersect_house, "_n")

  # Calculate mean feed intake for each common house and assign it to new columns
  res3[, paste0(intersect_house, "_mean_feed") := Map(function(feed, n) res3[[feed]] / res3[[n]],
                                                      cols_feed_name, cols_n_name)]

  # Step 3: Exclude non-numeric "_n" columns to focus on numeric analysis
  #numeric_cols <- setdiff(names(res3), grep("_n$", names(res3), value = TRUE))

  # Step 4: Isolate the data.table to numeric columns for further processing
  #res4 <- res3[, .(setdiff(names(res3), grep("_n$", names(res3), value = TRUE)))]

  # Identify and round double columns to three decimal places
  double_cols <- names(res3)[sapply(res3, is.double)]
  res3[, (double_cols) := lapply(.SD, function(x) round(x, digits = 3)), .SDcols = double_cols]

  return(res3)
}
house_weight <- function(data, house_width, days) {
  house <- location <- mean_weight <- weight <- . <- mean_weight_house <- NULL
  # Create a new column 'house' by concatenating "house_" with the first character of 'location'
  temp <- data[, house := paste0("house_", substr(location, 1, house_width))
               # Filter rows to include only those with 'date' within the last 7 days from 2023-11-01
  ][date >= lubridate::today() - days
    # Reformat the 'date' column to 'year-month-day'
  ][, date := format(date, "%y-%m-%d")
    # Calculate the mean 'weight' divided by 1000 for each 'location' and 'date', and create 'mean_weight' column
  ][, mean_weight := mean(weight)/1000, by = .(location, date)
    # Select specific columns 'date', 'house', 'location', and 'mean_weight' for the output
  ][, .(date, house, location, mean_weight)
    # Remove duplicate rows based on 'house', 'date', and 'location'
  ][, unique(.SD, by = c("house", "date", "location"))
    # Calculate the mean of 'mean_weight' for each 'house' and 'date', creating 'mean_weight_house'
  ][, mean_weight_house := mean(mean_weight), by = .(house, date)
    # Select specific columns 'date', 'house', and 'mean_weight_house' for the output
  ][, .(date, house, mean_weight_house)
    # Remove duplicate rows from the current selection
  ][, unique(.SD)
    # Reshape the data to a wide format, with 'date' as rows, 'house' as columns, and 'mean_weight_house' as values
  ][, dcast(.SD, date ~ house, value.var = "mean_weight_house")]

  # Identify numeric columns, excluding 'date'
  numeric_cols <- setdiff(names(temp), c("date"))
  # Round all numeric columns to two decimal places
  temp[, (numeric_cols) := lapply(.SD, function(x) round(x, digits = 2)), .SDcols = numeric_cols]
  # Return the processed data.table
  return(temp)
}
responder_na <- function(data, days) {
  visit_time <- responder <- n <- . <- location <- sum_na <- NULL
  # Convert 'visit_time' to IDate and ITime, then assign to 'date' and 'time' columns
  temp <- data[, `:=`(c("date", "time"), data.table::IDateTime(visit_time))
               # Filter rows to include only those with 'date' within the last 7 days
  ][date >= lubridate::today() - days
    # Remove duplicate rows based on all columns
  ][, unique(.SD)
    # Reformat 'date' column using 'visit_time' to 'year-month-day' format
  ][, date := format(visit_time, format="%y-%m-%d")
    # Filter rows where 'responder' is NA
  ][is.na(responder)
    # For each combination of 'location' and 'date', count the number of NAs in 'responder'
  ][, `:=`(n, .N), .(location, date)
    # Select 'location', 'date', and the count of NAs (n)
  ][, .(location, date, n)
    # Remove duplicate rows based on all columns again
  ][, unique(.SD)
    # Reshape the data to a wide format, with 'location' as rows, 'date' as columns, and 'n' as values
  ][, dcast(.SD, location ~ date, value.var = "n")
    # Ensure 'location' is treated as a character column
  ][, location := as.character(location)]

  # Identify numeric columns, excluding 'location'
  numeric_cols <- setdiff(names(temp), c("location"))
  # Sum up values across numeric columns for each row, ignoring NAs, and assign to 'sum_na'
  temp[, sum_na := rowSums(.SD, na.rm = TRUE), .SDcols = numeric_cols]

  # Return the processed data.table
  return(temp)
}
extreme_time_n <- function(data, days) {
  visit_time <- . <- location <- responder <- weight <- duration <- outlier <- is.extreme <- extreme_weight <- sum_time <- sum_n <- NULL
  # Split visit_time into date and time components
  data_temp <- data[, `:=`(c("date", "time"), data.table::IDateTime(visit_time))
                    # Filter records from the last 7 days
  ][date >= lubridate::today() - days
    # Remove duplicate rows
  ][, unique(.SD)
    # Reformat date in "yy-mm-dd" format
  ][, date := format(visit_time, format="%y-%m-%d")
    # Select only specific columns
  ][, .(location, date, responder, weight, duration)
    # Remove rows where location is NA
  ][!is.na(location)
  ]

  # Find outliers by location, date, and responder
  outlier_find <- data_temp[, .(location, date, responder, weight)
  ][, .(outlier = list(.SD)), by = .(location, date, responder)
    # Identify outliers using rstatix package
  ][, `:=`("outlier", purrr::map(.SD[[1]], \(x) rstatix::identify_outliers(x))), .SDcols = "outlier"
    # Bind rows of outliers into a single data.table
  ][, rbindlist(outlier), by = .(location, date, responder)]

  # Merge the original data with outlier findings
  data_merge <- merge(data_temp, outlier_find, all.x = TRUE)

  # Impute missing values for is.outlier and is.extreme as FALSE
  data_impute <- data_merge[, c("is.outlier", "is.extreme") :=
                              lapply(.SD, function(x) fifelse(is.na(x), FALSE, x)),
                            .SDcols = c("is.outlier", "is.extreme")]

  # Calculate the ratio of extreme values by location and date
  extreme_ratio <- data_impute[, .(extreme_weight = 100 * sum(is.extreme) / .N), by = .(location, date)
  ][, unique(.SD)
    # Filter for positive ratios
  ][extreme_weight > 0
    # Reshape data from long to wide format, setting True_Ratio by location and date
  ][, dcast(.SD, location~ date, value.var = "extreme_weight")]

  numeric_cols <- setdiff(names(extreme_ratio), c("location"))
  extreme_ratio[, (numeric_cols) := lapply(.SD, function(x) round(x, digits = 2)), .SDcols = numeric_cols]

  # Calculate total duration and count of visits by location and date
  feed_time_n <- data_impute[, sum_time := sum(duration)/3600, by = .(location, date)
  ][, `:=`(sum_n, .N), by = .(location, date)
  ][, .(location, date, sum_n, sum_time)
    # Remove duplicates
  ][, unique(.SD)
    # Reshape data from long to wide format, setting sum_time and sum_n by location and date
  ][, dcast(.SD, location~ date, value.var = c("sum_time", "sum_n"))]

  numeric_cols <- grep("time", names(feed_time_n), value = TRUE)
  feed_time_n[, (numeric_cols) := lapply(.SD, function(x) round(x, digits = 2)), .SDcols = numeric_cols]

  # Return a list of results
  return(list(extreme_ratio = extreme_ratio, feed_time_n = feed_time_n))
}
hour_stat <- function(data) {
  visit_time <- time <- location <- duration <- . <- n <- feedintake <- NULL
  # Split visit_time into date and time components
  data_temp <- data[, `:=`(c("date", "time"), data.table::IDateTime(visit_time))
                    # Filter records from the last 7 days
  ][date >= lubridate::today() - 1
    # Remove duplicate rows
  ][, unique(.SD)
    # Reformat date in "yy-mm-dd" format
  ][, date := format(visit_time, format="%y-%m-%d")
    # Select only specific columns
  ][, hour := as.integer(substr(time, 1, 2))][!is.na(location)
  ][, `:=`(n = .N, feed_time = sum(duration)/60, feedintake = sum(feed_intake)/1000), by = .(location, hour)
  ][, .(location, date, hour, n, feed_time, feedintake)
    # Remove rows where location is NA
  ][, unique(.SD)]

  numeric_cols <- grep("feed_time", names(data_temp), value = TRUE)
  data_temp <- data_temp[, (numeric_cols) := lapply(.SD, function(x) round(x, digits = 2)), .SDcols = numeric_cols]

  visit_n <- data_temp[, dcast(.SD, location ~ hour , value.var = "n")]
  feed_time <- data_temp[, dcast(.SD, location ~ hour , value.var = "feed_time")]
  feed_intake <- data_temp[, dcast(.SD, location ~ hour , value.var = "feedintake")]

  # Return a list of results
  return(list(visit_n = visit_n, feed_time = feed_time, feed_intake = feed_intake))
}
