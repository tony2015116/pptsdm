---
title: "flat_teaching.Rmd for working package"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{=html}
<!-- 
Run this 'development' chunk

Store every call to library() that you need to run chunks line by line, as in a classical Rmd for analysis
-->
```
```{r development, include=FALSE}
library(data.table)
library(ggplot2)
library(tidyfst)
library(robustbase)
library(taskscheduleR)
library(patchwork)
library(lubridate)
library(openxlsx)
```

```{=html}
<!--
# Description of your package

This will fill the description of your package.
Fill and run the content of this chunk, before anything else. 

Note: when you will use other flat templates, this part will be in a separate file. Do not be surprised!
-->
```
```{r description, eval=FALSE}
# Describe your package
fusen::fill_description(
  pkg = here::here(),
  fields = list(
    Title = "Breeding Tools",
    Description = "An R package that can monitor the csv data and stations from the Nedap in pig farm.",
     `Authors@R` = c(
      person("Guo Meng", email = "tony2015116@163.com", role = c("aut", "cre")),
      person(given = "Guo Meng", role = "cph")
    )
  ),
  overwrite=T
)
# Define License with use_*_license()
usethis::use_mit_license("Guo Meng")
```

# fid_monitor

You can use `fid_monitor()` to monitor the feed intake of each pig in the nedap or fire pig performance test station as well as the total feed intake of all pigs.

```{r function-fid_monitor}
#' Feed intake monitor of pig performance test station
#' 
#' @param data A data frame or data table containing the nedap or fire pig performance test data to be processed. Columns must include 'visit_time', 'location', 'responder', 'feed_intake'.
#' 
#' @param begin_date An optional Date object or character string specifying the beginning date for the data to be processed. If not provided, all dates in the data will be considered.
#' 
#' @param station_type A character string specifying the type of station. This must be either 'nedap' or 'fire'.
#' @param save_path A character string specifying the path where the output PDF will be saved.
#' 
#' @importFrom data.table ":="
#' @importFrom data.table ".SD"
#' 
#' @return This function does not return a value. It saves a PDF file to the specified path.
#' @export
#' 
fid_monitor <- function(data, begin_date=NULL, station_type, save_path) {
  if (missing(data)) stop("Please provide 'data' argument.")
  if (missing(station_type)) stop("Please provide 'station_type' argument.")
  if (missing(save_path)) stop("Please provide 'save_path' argument.")
  # Argument checks
  if (!is.data.frame(data) && !data.table::is.data.table(data)) {
    stop("The 'data' argument must be a data.frame or a data.table.")
  }
  
  if (!is.null(begin_date)) {
    if (!inherits(begin_date, "Date") && !is.character(begin_date)) {
      stop("Error: 'begin_date' argument must be a Date object or character string.")
    }
    
    if (is.character(begin_date)) {
      begin_date <- as.Date(begin_date)
    }
  }
  
  if (!is.character(station_type) || !(station_type %in% c("nedap", "fire"))) {
    stop("The 'station_type' argument must be either 'nedap' or 'fire'.")
  }
  
  if (!is.character(save_path) || !dir.exists(save_path)) {
    stop("The 'save_path' argument must be a valid directory path.")
  }
  
  visit_time <- . <- location <- responder <- feed_intake <- all_feed_a_station_one_day <- total_intake <- percent_intake <- Date <- Consumed <- total_intake <- percent_intake <- ndt <- plot1 <- plot2 <- NULL
  
  if (!data.table::is.data.table(data)) {
    data <- data.table::setDT(data)
  }
  prepare_nedap_data <- function(data, begin_date = NULL) {
    temp1 <- unique(data)[,`:=`(c("date", "time"),data.table::tstrsplit(visit_time," ", fixed = TRUE))][, `:=`(c("date"), lubridate::ymd(date))][,!c("visit_time", "time")]
    temp2 <- temp1[, keyby = .(location, responder, date),.(total_intake = round(sum(feed_intake) / 1000, digits = 4))][,`:=`(all_feed_a_station_one_day, sum(total_intake)), by = .(location, date)][, `:=`(percent_intake, total_intake / all_feed_a_station_one_day)]
    to_factor = c("location", "responder")
    temp2[, `:=`((to_factor), purrr::map(.SD, as.factor)), .SDcols = to_factor]
    if (!is.null(begin_date)) {
      temp2 <- temp2[date >= begin_date]
    }
    return(temp2)
  }
  
  prepare_fire_data <- function(data, begin_date = NULL) {
    temp1 <- unique(data)[,`:=`(Date, lubridate::ymd(Date))]
    data.table::setnames(temp1, 1:3, c("location", "responder","date"))
    temp2 <- temp1[, keyby = .(location, responder, date),.(total_intake = round(sum(Consumed), digits = 4))][,`:=`(all_feed_a_station_one_day, sum(total_intake)),by = .(location, date)][, `:=`(percent_intake, total_intake / all_feed_a_station_one_day)]
    to_factor = c("location", "responder")
    temp2[, `:=`((to_factor), purrr::map(.SD, as.factor)),.SDcols = to_factor]
    if (!is.null(begin_date)) {
      temp2 <- temp2[date >= begin_date]
    }
    return(temp2)
  }
  
  # Prepare data based on station_type
  if (station_type == "nedap") {
    prepared_data <- prepare_nedap_data(data, begin_date = begin_date)
  } else if (station_type == "fire") {
    prepared_data <- prepare_fire_data(data, begin_date = begin_date)
  } else {
    stop("Invalid station_type. Supported types are 'nedap' and 'fire'.")
  }
  
  # Create the plots
  colors = c("#a6cee3", "#1f78b4", "#b2df8a", "#33a02c", "#fb9a99", "#e31a1c", "#fdbf6f", "#ff7f00", "#cab2d6", "#6a3d9a", "#b15928", "#8dd3c7", "#d9d9d9", "#80b1d3", "#00AFBB", "#01665e", "#003c30", "blue", "pink", "yellow", "red", "green", "#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#00AFBB", "#E7B800", "#FC4E07", "#1B9E77", "#D95F02", "#7570B3", "#E7298A", "#66A61E", "#E6AB02", "#A6761D", "#666666", "purple")
  #colors = as.character(palette.colors(n = 36, "Polychrome 36"))
  
  create_plots <- function(data, ...) {
    data[, list(list(.SD)), by = location
    ][, `:=`("plot1", purrr::map2(.SD[[1]], location, ~ ggplot2::ggplot(
      data = .x,
      ggplot2::aes(y = percent_intake,
                   x = date, fill = responder)
    ) + ggplot2::theme_bw() +
      ggplot2::geom_col(
        width = 0.8,
        na.rm = F,
        show.legend = T
      ) +
      ggplot2::ggtitle(.y) + ggplot2::scale_y_continuous(
        labels = scales::percent,
        limits = c(0, 1),
        breaks = seq(0, 1, 0.1)
      ) + ggplot2::theme(
        plot.title = ggplot2::element_text(
          color = "black",
          hjust = 0.5,
          size = 20
        ),
        axis.title.x = ggplot2::element_blank(),
        axis.title.y = ggplot2::element_text(size = 15),
        axis.text.x = ggplot2::element_text(size = 8, angle = 270),
        axis.text.y = ggplot2::element_text(size = 8),
        legend.title = ggplot2::element_text(size = 8),
        legend.text = ggplot2::element_text(size = 8),
        legend.position = "top",
        legend.margin = ggplot2::margin(10, 10, 10, 10),  # Adjust the legend margin
        legend.box.margin = ggplot2::margin(0, 0, 30, 0)  # Extra space at the bottom for the legend
      ) +
      ggplot2::guides(
        shape = ggplot2::guide_legend(override.aes = list(size = 7)),
        color = ggplot2::guide_legend(override.aes = list(size = 7))
      ) +
      ggplot2::ggtitle(paste0("location:", .y)) + ggplot2::scale_x_date(
        date_breaks = "1 day",
        date_labels = "%m-%d",
        date_minor_breaks = "1 day"
      ) +
      ggplot2::scale_fill_manual(
        na.value = "black",
        values = colors
      ), ...)), .SDcols = "V1"
    ][, `:=`("plot2", purrr::map2(.SD[[1]], location, ~ ggplot2::ggplot(data = .x,
                                                                        ggplot2::aes(
                                                                          y = total_intake, x = date, fill = responder
                                                                        )) +
                                    ggplot2::theme_bw() + ggplot2::geom_col(
                                      width = 0.8,
                                      na.rm = F,
                                      show.legend = F
                                    ) + ggplot2::theme(
                                      plot.title = ggplot2::element_text(
                                        color = "black",
                                        hjust = 0.5,
                                        size = 20
                                      ),
                                      axis.title.x = ggplot2::element_text(size = 15),
                                      axis.title.y = ggplot2::element_text(size = 15),
                                      axis.text.x = ggplot2::element_text(size = 8, angle = 270),
                                      axis.text.y = ggplot2::element_text(size = 8),
                                      legend.position = "none"
                                    ) +
                                    ggplot2::guides(
                                      shape = ggplot2::guide_legend(override.aes = list(size = 7)),
                                      color = ggplot2::guide_legend(override.aes = list(size = 7))
                                    ) +
                                    ggplot2::scale_x_date(
                                      date_breaks = "1 day",
                                      date_labels = "%m-%d",
                                      date_minor_breaks = "1 day"
                                    ) + ggplot2::scale_fill_manual(
                                      na.value = "black",
                                      values = colors
                                    ), ...)), .SDcols = "V1"][]
  }
  
  # Combine and save the plots
  save_combined_plots <- function(path_out, ...) {
    temp_plot <- create_plots(data = prepared_data, value = colors)
    
    # Calculate the number of unique locations
    num_locations <- length(unique(temp_plot$location))
    
    # Calculate the date range
    date_range <- range(prepared_data$date)
    num_days <- as.numeric(difftime(date_range[2], date_range[1], units = "days")) + 1
    
    # Set the minimum days for width calculation to 30
    num_days_for_width <- max(num_days, 50)
    
    # Calculate the PDF dimensions based on the number of locations and date range
    width_per_day <- 0.3 # 0.5 cm per day, you can adjust this value based on your preferences
    pdf_width <- num_days_for_width * width_per_day * 2 # Adjust width according to the date range and number of columns
    
    height_per_location <- 30 # 30 cm per location, you can adjust this value based on your preferences
    pdf_height <- height_per_location * ceiling(num_locations / 2) # Adjust height according to the number of locations and number of rows
    
    temp4 <- temp_plot[, `:=`("finals", purrr::pmap(.(.SD[[1]], .SD[[2]]), function(x, y) patchwork::wrap_plots(x, y, ncol = 1))), .SDcols = c("plot1","plot2")][]
    temp5 <- patchwork::wrap_plots(temp4$finals, ncol = 2)
    
    ggplot2::ggsave(
      file.path(save_path, "feed_intake_monitor.pdf"),
      temp5,
      width = pdf_width,
      height = pdf_height,
      units = "cm",
      dpi = "retina",
      limitsize = FALSE,
      ...
    )
  }
  save_combined_plots(save_path)
}
```

```{r examples-fid_monitor, warning=FALSE}
# Load CSV data
data <- data.table::fread("C:/Users/Dell/Documents/projects/pptsdm_data/ppt_monitor_test_data.csv")
# Feed intake monitor
fid_monitor(data = data, station_type = "nedap", save_path = "C:/Users/Dell/Downloads/test")
```

```{=html}
<!-- 
This first section shows:

- the three parts necessary for a package: 'function', 'examples' and 'tests'.  
  + Note that the three following chunks have names accordingly.

-->
```
```{=html}
<!--
Here is an example on how to use the function.
This should be a reproducible and working example
-->
```
# station_monitor

You can use function of `station_monitor()` to monitor pig performance test data from nedap and fire pig performance test stations. This function can monitor the changes in the number of pigs, total visitation time of pigs, total number of visits, changes in total feed intake, and also monitor the changes in pig weights for a specific pig performance test station.

```{r function-station_monitor}
#' Feed intake monitor of pig performance test station
#' 
#' @param data A data frame or data table. This is the data to be processed. It should have specific columns depending on the station type.
#' @param begin_date An optional parameter. If provided, only data from this date onwards will be considered. It can be a Date object or a character string in the form 'yyyy-mm-dd'. Default is NULL, which means all dates in the data will be considered.
#' @param station_type A character string specifying the type of station. This must be either 'nedap' or 'fire'.
#' @param save_path A character string specifying the path where the output PNG files will be saved.
#' 
#' @importFrom data.table ":=" "CJ" ".SD"
#' 
#' @return This function does not return a value. It saves PNG files to the specified path.
#' @export
#' 
station_monitor <- function (data, begin_date=NULL, station_type, save_path)
{
  if (missing(data)) stop("Please provide 'data' argument.")
  if (missing(station_type)) stop("Please provide 'station_type' argument.")
  if (missing(save_path)) stop("Please provide 'save_path' argument.")
  # Argument checks
  if (!is.data.frame(data) && !data.table::is.data.table(data)) {
    stop("The 'data' argument must be a data.frame or a data.table.")
  }
  
  if (!is.null(begin_date)) {
    if (!inherits(begin_date, "Date") && !is.character(begin_date)) {
      stop("Error: 'begin_date' argument must be a Date object or character string.")
    }
    
    if (is.character(begin_date)) {
      begin_date <- as.Date(begin_date)
    }
  }
  
  if (!is.character(station_type) || !(station_type %in% c("nedap", "fire"))) {
    stop("The 'station_type' argument must be either 'nedap' or 'fire'.")
  }
  
  if (!is.character(save_path) || !dir.exists(save_path)) {
    stop("The 'save_path' argument must be a valid directory path.")
  }
  visit_time <-
    responder <-
    . <-
    location <-
    animal_number <-
    duration <-
    feed_intake <-
    Entry <-
    Exit <- Consumed <- weight <- ndt <- . <- items <- plot1 <- .N <- V1 <- hour <- plot_name <- NULL
  
  if (!data.table::is.data.table(data)) {
    data <- data.table::setDT(data)
  }
  # Prepare data based on station_type
  prepare_nedap_data <- function(data, begin_date = NULL) {
    temp1 <-
      unique(data)[, `:=`(c("date", "time"),data.table::tstrsplit(visit_time," ", fixed = TRUE))
      ][, `:=`(c("date"), data.table::as.IDate(date))
      ][,!c("visit_time", "time")]
    if (!is.null(begin_date)) {
      temp1 <- temp1[date >= begin_date]
    }
    temp2 <- unique(temp1, by = c("location", "responder", "date"))[!is.na(responder)
    ][, keyby = .(location,date), .(animal_number = .N)]
    temp3 <- temp1[!is.na(animal_number)
    ][, keyby = .(location,date), .(`total_intake_duration(min)` = round(sum(duration) / 60,digits = 4),total_intake = round(sum(feed_intake) / 1000,digits = 4),visit_number = .N)]
    temp5 <- merge(temp2, temp3, all.x = TRUE)
    list(temp5 = temp5, temp1 = temp1)
  }
  
  prepare_fire_data <- function(data, begin_date = NULL) {
    temp1 <- unique(data)[, `:=`(Entry,do.call(paste, c(.SD, sep = " "))), .SDcol = c("Date","Entry")
    ][, `:=`(Exit, do.call(paste, c(.SD, sep = " "))),.SDcol = c("Date", "Exit")
    ][, `:=`(c("Entry", "Exit"),lapply(.SD, lubridate::ymd_hms)), .SDcol = c("Entry","Exit")
    ][, `:=`(duration,data.table::fifelse(Exit -Entry < 0 & lubridate::hour(Exit) == 0,Exit - Entry + lubridate::ddays(1), Exit - Entry))]
    data.table::setnames(temp1,
                         c(1:3, 9),
                         c("location",
                           "responder", "date", "weight"))
    temp1 <- unique(temp1)[, `:=`(date, lubridate::ymd(date))]
    if (!is.null(begin_date)) {
      temp1 <- temp1[date >= begin_date]
    }
    temp2 <- unique(temp1, by = c("location", "responder","date"))[!is.na(responder)
    ][, keyby = .(location, date), .(animal_number = .N)]
    temp3 <- temp1[!is.na(responder)
    ][, `:=`(duration, as.numeric(duration))
    ][,keyby = .(location, date), .(`total_intake_duration(min)` = round(sum(duration) / 60,digits = 4), total_intake = round(sum(Consumed),digits = 4),visit_number = .N)]
    temp5 <- merge(temp2, temp3, all.x = TRUE)
    list(temp5 = temp5, temp1 = temp1)
  }
  
  
  # Prepare data based on station_type
  if (station_type == "nedap") {
    prepared_data_list <- prepare_nedap_data(data, begin_date = begin_date)
    prepared_data <- prepared_data_list$temp5
    temp1 <- prepared_data_list$temp1
  } else if (station_type == "fire") {
    prepared_data_list <- prepare_fire_data(data, begin_date = begin_date)
    prepared_data <- prepared_data_list$temp5
    temp1 <- prepared_data_list$temp1
  } else {
    stop("Invalid station_type. Supported types are 'nedap' and 'fire'.")
  }
  
  # The rest of the code remains the same.
  # 将第3到第6列的类型转换为numeric
  prepared_data[, (3:6) := lapply(.SD, as.numeric), .SDcols = 3:6]
  
  # 使用melt函数
  temp6 = data.table::melt(prepared_data,
                           id.vars = c("location", "date"),
                           measure.vars = 3:6,
                           variable.name = "items",
                           value.name = "values")
  
  temp6_2 <- temp1[!is.na(location), .(location, date, weight)
  ][, list(list(.SD)), by = location # nest by location
  ][, `:=`("V1", purrr::map(.SD[[1]], function(data) {data[data.table::CJ(date = tidyr::full_seq(date, 1)), on = .(date)
  ][data.table::CJ(date = date, unique = TRUE), on = .(date)]})), .SDcols = "V1"
  ][, `:=`("plot1", purrr::map2(.SD[[1]], location, ~ggplot2::ggplot(data = .x, ggplot2::aes(x = date, y = weight, group = date)) +
                                  ggplot2::geom_boxplot(outlier.color = "red") +
                                  ggplot2::scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
                                  cowplot::background_grid(minor = "none") +
                                  ggplot2::scale_x_date(date_breaks = "1 day", date_labels = "%d") +
                                  ggplot2::theme_bw() +
                                  ggplot2::theme(legend.position = "none",
                                                 axis.title.x = ggplot2::element_blank(),
                                                 axis.text.x = ggplot2::element_text(angle = -90),
                                                 axis.text = ggplot2::element_text(size = 8)))), .SDcols = "V1"
  ][ , V1 := NULL
  ][]
  
  temp7 <- temp6[, list(list(.SD)), by = location
  ][, `:=`("V1", purrr::map(.SD[[1]], function(data) {data[CJ(date = tidyr::full_seq(date, 1)), on = .(date)
  ][CJ(date = date, items = items, unique = TRUE), on = .(date, items)
  ][!is.na(items)
  ][,`:=`(items, factor(items, labels = c("N", "visit_time/min","feed_intake/kg", "visit_number")))]})), .SDcols = "V1"
  ][, `:=`("plot2", purrr::map2(.SD[[1]], location,~ggplot2::ggplot(data = .x, ggplot2::aes(x = date, y = values)) +
                                  ggplot2::geom_point(ggplot2::aes(col = items)) +
                                  ggplot2::geom_line(ggplot2::aes(col = items)) +
                                  ggplot2::theme_bw() +
                                  ggplot2::facet_grid(items ~ ., scales = "free") +
                                  ggplot2::scale_x_date(date_breaks = "1 day", date_labels = "%m") +
                                  ggplot2::scale_colour_brewer(palette = "Set1") +
                                  ggplot2::theme(strip.text.y = ggplot2::element_text(angle = 0,hjust = 0),
                                                 legend.position = "none",
                                                 axis.title = ggplot2::element_blank(),
                                                 axis.text.x = ggplot2::element_text(angle = -90),
                                                 axis.text = ggplot2::element_text(size = 8),
                                                 strip.placement = "outside",
                                                 strip.background = ggplot2::element_rect(colour = "white", fill = "white")) +
                                  ggplot2::ggtitle(paste0("Location:", .y)) +
                                  cowplot::background_grid(minor = "none"))), .SDcols = "V1"][]
  
  temp8 <- merge(temp7, temp6_2, by = "location", all.x = TRUE)
  temp9 <- temp8[, `:=`("finals", purrr::pmap(.(.SD[[1]], .SD[[2]]), function(x, y) patchwork::wrap_plots(x, y, nrow = 2))), .SDcols = c("plot2","plot1")
  ][, plot_name := paste0(location, "_stations.png")][]
  
  # 根据天数计算图片的长度和宽度
  # 使用 map() 遍历 V1 列中的所有列表，计算唯一日期数
  days <- purrr::map_dbl(temp9$V1, ~ length(unique(.x$date)))
  height <- ifelse(days <= 7, 8, ifelse(days <= 14, 8, ifelse(days <= 30, 8, 8)))
  width <- ifelse(days <= 7, 5, ifelse(days <= 14, 10, ifelse(days <= 30, 13, 16)))
  
  purrr::walk2(
    temp9$plot_name,
    temp9$finals,
    ~ ggplot2::ggsave(
      filename = file.path(save_path, .x),
      plot = .y,
      height = height,
      width = width
    )
  )
}
```

```{r examples-station_monitor}
# Load CSV data
data <- data.table::fread("C:/Users/Dell/Documents/projects/pptsdm_data/ppt_monitor_test_data.csv")
# Station monitor
station_monitor(data = data, station_type = "nedap", save_path = "C:/Users/Dell/Downloads/test")
```

# process_data
    
```{r function-process_data}
#' Process Data Frame or Data Table
#' 
#' This function performs various checks and transformations on a provided data frame or data table.
#' It checks for necessary columns, correct data types, and performs various operations like filtering, 
#' joining, and calculating sequences.
#' 
#' @param data A data frame or data table that must contain specific columns with specific data types.
#'             Required columns are: "animal_number", "lifenumber", "responder", "location", "visit_time",
#'             "duration", "state", "weight", "feed_intake".
#'
#' @return Returns a modified data table with computed sequence features and potentially other transformations.
#'         The function ensures that columns are of the correct type and that data integrity is maintained.
#' 
#' @export
#' 
#' @note This function will stop execution and produce an error if the input data does not meet the expected
#'       requirements, such as missing columns or incorrect data types.
process_data <- function(data) {
  if (missing(data)) stop("Missing data frame or data table!")
  if (!is.data.frame(data) && !inherits(data, "data.table")) stop("Data is not a data frame or data table!")
  #if (!is.data.frame(data) && !class(data) == 'data.table')
  # Check for required columns
  required_columns <- c("animal_number", "lifenumber", "responder", "location", "visit_time", "duration", "state", "weight", "feed_intake")
  missing_columns <- setdiff(required_columns, names(data))
  if (length(missing_columns) > 0) stop(paste("Missing columns:", paste(missing_columns, collapse = ", ")))

  # Check types for some columns
  if (!is.numeric(data$animal_number) && !is.character(data$animal_number)) stop("'animal_number' must be numeric or character!")
  if (!is.logical(data$lifenumber) && !is.character(data$lifenumber)) stop("'lifenumber' must be logical or character!")
  if (!is.numeric(data$responder) && !is.character(data$responder)) stop("'responder' must be numeric or character!")
  if (!is.numeric(data$location)) stop("'location' must be numeric!")
  if (!is.character(data$visit_time) && !inherits(data$visit_time, "POSIXt")) stop("'visit_time' must be character or POSIXct!")
  if (!is.numeric(data$duration)) stop("'duration' must be numeric!")
  if (!is.numeric(data$state)) stop("'state' must be numeric!")
  if (!is.numeric(data$weight)) stop("'weight' must be numeric!")
  if (!is.numeric(data$feed_intake)) stop("'feed_intake' must be numeric!")

  # Check if the data is a data.frame, if yes, then make a deep copy of the data and convert it into data.table
  if (is.data.frame(data)) data <- data.table::as.data.table(data.table::copy(data))

  responder <- weight <- . <- location <- N <- n <- location_maxn <- visit_time <- seq_days <- seq_in_day <- seq_in_location <- feed_intake <- NULL

  # Filter out the data with NA in 'responder' column and remove duplicates
  data_temp <- unique(data)[!is.na(responder)]

  # Create a unique data.table for 'responder' and 'location'
  unique_dt <- unique(data_temp[, .(responder, location)])

  # Find duplicate 'responder's
  dup_responders <- unique_dt[, .N, by = .(responder)][N > 1]

  # Compute the number of records for each 'responder' and 'location'
  num_records <- unique(data_temp[, `:=`(n, .N), .(responder, location)][, .(responder, location, n)])

  # Set 'responder' as the key for join operations
  data.table::setkey(dup_responders, responder)
  data.table::setkey(num_records, responder)

  # Perform left join operation on 'num_records' and 'dup_responders'
  dup_records <- num_records[dup_responders]

  # Modify the 'location' in the unique data.table for duplicate 'responder's
  if(nrow(dup_responders) > 0) {
    # Compute the 'location' with maximum number of records for each 'responder'
    max_n_location <- num_records[, .(max_n = max(n), location_maxn = location[which.max(n)]), by = responder]

    # Remove duplicates in 'max_n_location' after modifying 'location'
    max_n_location <- unique(max_n_location)

    # Perform left join operation on 'data_temp' and 'max_n_location' and update 'location' to 'location_maxn'
    data_temp <- merge(data_temp, max_n_location, by = "responder", all.x = TRUE)[, location := location_maxn][, c("max_n", "location_maxn") := NULL]
  }

  # Preprocess data and compute sequence features
  # Check the class of visit_time
  if(is.character(data_temp$visit_time)) {
    # If visit_time is a character vector, replace "/" with "-"
    data_temp[, visit_time := gsub("/", "-", visit_time)]
    data_pre <- data_temp[, `:=`(c("date", "time"), data.table::IDateTime(visit_time))]
  } else {
    # If visit_time is not a character vector, assume it's a datetime object
    data_pre <- data_temp[, `:=`(c("date", "time"), data.table::IDateTime(visit_time))]
  }
  data_pre <- data_pre[data.table::CJ(date = tidyr::full_seq(date, 1)), on = .(date)  # Compute complete sequence of dates
  ][order(date), `:=`(seq_days, .GRP), by = date  # Compute sequence number of days
  ][order(visit_time), `:=`(seq_in_day, 1:.N), by = .(responder, date)  # Compute sequence number in day
  ][order(visit_time), `:=`(seq_in_location, 1:.N), by = .(location, date)  # Compute sequence number in location
  ][order(responder, visit_time)  # Order data by 'responder' and 'visit_time'
  ][, .(responder, location, date, seq_days, seq_in_day, seq_in_location, feed_intake, weight)  # Keep only necessary columns #seq_in_location, seq_days, seq_in_day,, weight
  ][, `:=` (responder = as.character(responder),  # Convert 'responder' and 'location' to character type
            location = as.character(location),
            feed_intake = as.numeric(feed_intake),
            weight = as.numeric(weight))][!is.na(location)][]  # Convert 'weight' to numeric type
  return(data_pre)
}
```
  
```{r example-process_data}
# Load CSV data
data <- data.table::fread("C:/Users/Dell/Documents/projects/pptsdm_data/ppt_monitor_test_data.csv")
processed_data <- process_data(data = data)
head(processed_data)
```
  
  
# house_weight
    
```{r function-house_weight}
#' Calculate Mean Weight for Specified Location and Date Range
#' 
#' This function processes a dataset to extract weights by specified house (part of location), 
#' calculates mean weights per house over a given number of days from a reference date, and 
#' returns the data in a wide format with dates as rows and houses as columns.
#' 
#' @param data A data frame that must include the columns: "location", "date", and "weight".
#' @param house_width A string indicating the number of characters to use from the 'location'
#'                    column to create the 'house' identifier. Defaults to "1".
#' @param days An integer specifying the number of days to include in the analysis
#'                  back from the reference date.
#' @param ref_date A Date object used as the reference date for filtering data. 
#'                 Defaults to the current system date.
#'
#' @return Returns a data frame in wide format where each column represents a house
#'         and each row represents a date within the specified range. Values are mean weights
#'         (in kilograms, rounded to two decimal places) for each house-date combination.
#' 
#' @export
#'
#' @note If 'data' does not contain the necessary columns, the function will stop with an error message.
#'       It is crucial to ensure that the 'location' and 'date' columns are formatted correctly.
house_weight <- function(data, house_width = "1", days, ref_date = Sys.Date()) {
  house <- location <- mean_weight <- weight <- . <- NULL
  data <- process_data(data)

  # Validate necessary columns presence
  necessary_cols <- c("location", "date", "weight")
  if (!all(necessary_cols %in% names(data))) {
    stop("Data must include the necessary columns: location, date, and weight.")
  }
  
  # Create 'house' column by extracting specified width from 'location'
  data[, house := paste0("house_", substr(location, 1, house_width))]

  # Filter data for the last specified number of days from a reference date
  filtered_data <- data[date >= (ref_date - days) & date < ref_date, ]

  # Calculate mean weight per location and date, adjust weight scale
  stat_data <- filtered_data[, mean_weight := mean(weight) / 1000, by = .(house, date)
  ][, .(date, house, mean_weight)
  ][, unique(.SD)][, date := format(date, "%m-%d")]

  # Remove duplicate entries and reshape data to wide format
  result_data <- dcast(stat_data, date ~ house, value.var = "mean_weight")

  # Round numeric columns to two decimal places
  numeric_cols <- setdiff(names(result_data), "date")
  result_data[, (numeric_cols) := lapply(.SD, round, digits = 2), .SDcols = numeric_cols][]

  return(result_data)
}
```
  
```{r example-house_weight}
# Load CSV data
data <- data.table::fread("C:/Users/Dell/Documents/projects/pptsdm_data/ppt_monitor_test_data.csv")
head(house_weight(data = data, days = 5))
```
  
# responder_na
    
```{r function-responder_na}
#' Analyze Missing Responder Data
#'
#' This function processes a dataset to identify and summarize the occurrences of NA values in the
#' 'responder' column within a specified number of days from a reference date. It converts dates,
#' removes duplicates, and aggregates NA counts per location and date. The output is presented in
#' a wide format with locations as rows and formatted dates as columns.
#'
#' @param data A data table that must include the columns: 'location', 'date', 'visit_time', and 'responder'.
#' @param days An integer specifying the number of days to include in the analysis back from the reference date.
#' @param ref_date A Date object used as the reference date for filtering data; defaults to the current system date.
#'
#' @return Returns a data table in wide format where each row represents a location and each column a date.
#'         Entries are the counts of NAs found in the 'responder' column for that location and date. 
#'         An additional 'total_nas' column shows the sum of NAs across all dates for each location.
#'
#' @export
#' 
#' @note This function stops execution and produces an error if the input data does not contain all the required columns.
#'       Ensure that 'location', 'date', 'visit_time', and 'responder' columns are formatted correctly and present.
responder_na <- function(data, days, ref_date = Sys.Date()) {
  visit_time <- n <- responder <- . <- location <- sum_n <- total_nas <- NULL
  data <- copy(data)
  # Convert visit_time to date and time columns and filter data by the given date range
  processed_data <- data[, `:=`(c("date", "time"), data.table::IDateTime(visit_time))
  ][date >= (ref_date - days) & date < ref_date
  ][, unique(.SD)  # Remove duplicate entries
  ][, n := fifelse(is.na(responder), 1L, 0L), by = .(location, date)  # Count NAs in responder column
  ][, sum_n := sum(n), by = .(location, date)  # Sum the NA counts by location and date
  ][, date := format(date, format="%y-%m-%d")  # Format date for consistency in output
  ][, .(location, date, sum_n)  # Select necessary columns
  ][, unique(.SD)  # Remove any duplicates after processing
  ]

  # Reshape the processed data to wide format using location as rows and dates as columns
  wide_data <- dcast(processed_data, location ~ date, value.var = "sum_n")
  wide_data[, location := as.character(location)]  # Convert 'location' to character for consistent data types

  # Calculate the total number of NAs for each location across all date columns
  numeric_cols <- setdiff(names(wide_data), "location")
  wide_data[, total_nas := rowSums(.SD, na.rm = TRUE), .SDcols = numeric_cols][]

  return(wide_data)
}
```
  
```{r example-responder_na}
# Load CSV data
data <- data.table::fread("C:/Users/Dell/Documents/projects/pptsdm_data/ppt_monitor_test_data.csv")
print(responder_na(data = data, days = 5))
```
  
# extreme_time_n
    
```{r function-extreme_time_n}
#' Compute Outlier Statistics and Time-based Metrics
#'
#' This function analyzes weight data to identify outliers using the Interquartile Range (IQR) method,
#' and computes time-based metrics for data entries within a specified number of days from a reference date.
#' It also reshapes the results into a wide format, separating the percentage of weight outliers and other metrics
#' like count of observations and total duration by location and date.
#'
#' @param data data.table, required.
#'   The input data table which must contain the following columns: `visit_time` (POSIXct), `location` (character),
#'   `responder` (character), `weight` (numeric, in grams), and `duration` (numeric, in seconds).
#' @param days integer, required.
#'   The number of days before the reference date to include in the analysis.
#' @param ref_date Date, optional.
#'   The specific date to use as the reference for filtering data; defaults to the current system date.
#'
#' @return A list of two data.tables:
#' \itemize{
#'   \item `weight_outlier`: A data table in wide format showing the percentage of weight outliers
#'                           per location across the specified dates.
#'   \item `feedintake`: A data table in wide format showing the count of observations and total
#'                       duration in hours per location across the specified dates.
#' }
#' 
#' @export
#' 
#' @note
#' The function assumes that the input data has all the required columns. The 'visit_time' should
#' be in POSIXct format to correctly convert to dates. Make sure the data is pre-cleaned to avoid errors
#' due to missing values, especially in the 'location' and 'responder' columns.
extreme_time_n <- function(data, days, ref_date = Sys.Date()) {
  visit_time <- location <- . <- responder <- weight <- duration <- outlier <- NULL
  data <- data.table::copy(data)
  # Ensure 'visit_time' is a proper date-time format
  data_temp <- data[, `:=`(c("date", "time"), data.table::IDateTime(visit_time))
  ][date >= (ref_date - days) & date <= ref_date
  ][, unique(.SD)
  ][, date := format(date, format = "%m-%d")
  ][!is.na(location), .(location, date, responder, weight, duration)][!is.na(responder)]

  # Calculate Interquartile Range (IQR) and identify outliers
  data_temp[, outlier := {
    Q1 <- quantile(weight, 0.25, na.rm = TRUE)
    Q3 <- quantile(weight, 0.75, na.rm = TRUE)
    IQR <- Q3 - Q1
    (weight < (Q1 - 1.5 * IQR)) | (weight > (Q3 + 1.5 * IQR))
  }, by = .(location, date, responder)]

  # Compute statistics for each location and date
  extreme_stats <- data_temp[, .(
    extreme_weight = 100 * sum(outlier) / .N,   # Percentage of outliers
    n = .N,                                 # Total count of observations
    time = sum(duration) / 3600             # Total duration in hours
  ), by = .(location, date)]

  # Reshape data to wide format
  extreme_stats_wide <- dcast(extreme_stats, location ~ date, value.var = "extreme_weight")
  n_time_wide <- dcast(extreme_stats, location ~ date, value.var = c("n", "time"))

  # Round numeric columns to two decimal places
  numeric_cols <- setdiff(names(extreme_stats_wide), c("location"))
  extreme_stats_wide[, (numeric_cols) := lapply(.SD, round, digits = 2), .SDcols = numeric_cols]
  numeric_cols <- grep("time", names(n_time_wide), value = TRUE)
  n_time_wide[, (numeric_cols) := lapply(.SD, round, digits = 2), .SDcols = numeric_cols]
  return(list(weight_outlier = extreme_stats_wide, feedintake = n_time_wide))
}
```
  
```{r example-extreme_time_n}
# Load CSV data
data <- data.table::fread("C:/Users/Dell/Documents/projects/pptsdm_data/ppt_monitor_test_data.csv")
print(extreme_time_n(data = data, days = 5))
```
  
  
# low_feedintake
    
```{r function-low_feedintake}
#' Analyze Low Feed Intake
#'
#' This function processes a dataset to calculate the total feed intake per location, date,
#' and responder. It then identifies days where the sum of feed intake is below a threshold,
#' specifically less than 0.5 kg. The results are returned in a wide format with each location
#' and responder combination as a row and dates as columns.
#'
#' @param data A data table that must include the columns: 'location', 'date', 'responder', 
#'             and 'feed_intake'. The 'visit_time' must be able to be converted to IDateTime format.
#' @param days An integer specifying the number of days to include in the analysis up to and including
#'             the reference date.
#' @param ref_date A Date object used as the reference date for filtering data; defaults to the current
#'                 system date.
#'
#' @return Returns a data table in wide format where each row represents a combination of location and
#'         responder, columns are dates, and values are the summed feed intake that are below the threshold
#'         of 0.5 kg. Also includes a 'sum_feedintake' column which is the total sum of feed intake across
#'         the period for each row.
#' 
#' @export
#' 
#' @note This function assumes that the input data has been preprocessed to include the necessary columns.
#'       It will stop and raise an error if the 'visit_time' cannot be converted to date and time format.
low_feedintake <- function(data, days, ref_date = Sys.Date()) {
  visit_time <- . <- feed_intake <- location <- responder <- sum_feedintake <- NULL
  data <- data.table::copy(unique(data))
  processed_data <- data[, `:=`(c("date", "time"), data.table::IDateTime(visit_time))
  ][date >= (ref_date - days) & date <= ref_date
  ][, .(sum_feedintake = sum(feed_intake) / 1000), by = .(location, date, responder)][
    , date := format(date, "%m-%d")]

  # Filter entries where 'sum_feedintake' is less than 0.5
  processed_data <- processed_data[sum_feedintake < 0.5]

  # Convert to wide format
  data_wide <- dcast(processed_data, location + responder ~ date, value.var = "sum_feedintake")
  # Calculate sum across numeric columns for each row and round the results
  numeric_cols <- setdiff(names(data_wide), c("location", "responder"))
  res <- data_wide[, sum_feedintake := round(rowSums(.SD, na.rm = TRUE), digits = 2), .SDcols = numeric_cols][]
  # Sort by 'sum_feed_intake'
  setorder(res, responder, -sum_feedintake, na.last = TRUE)  # Sort in descending order

  return(res)
}
```
  
```{r example-low_feedintake}
# Load CSV data
data <- data.table::fread("C:/Users/Dell/Documents/projects/pptsdm_data/ppt_monitor_test_data.csv")
print(low_feedintake(data = data, days = 5))
```
  
# all_feedintake
    
```{r function-all_feedintake}
#' Calculate Total Feed Intake Per Location
#'
#' This function aggregates total feed intake across a specified date range for each location.
#' The feed intake data is first summed per location and date, then transformed to kilograms
#' and formatted to wide format with locations as rows and dates as columns.
#'
#' @param data A data table that must include at least 'location', 'date', 'visit_time', 
#'             and 'feed_intake' columns. 'visit_time' should be convertible to date and time.
#' @param days An integer specifying the number of days to include in the analysis up to and including
#'             the reference date.
#' @param ref_date A Date object used as the reference date for filtering data; defaults to the current
#'                 system date.
#'
#' @return Returns a data table in wide format with locations as rows and formatted dates as columns.
#'         Each entry is the summed feed intake for that date and location, converted to kilograms.
#'         An additional 'all_feedintake' column is included, showing the total feed intake for each
#'         location across all dates.
#' 
#' @export
#' 
#' @note This function expects that the input data is pre-cleaned and correctly formatted. 
#'       Errors may occur if 'visit_time' cannot be properly converted.
all_feedintake <- function(data, days, ref_date = Sys.Date()) {
  visit_time <- . <- feed_intake <- location <- NULL
  data <- data.table::copy(unique(data))
  processed_data <- data[, `:=`(c("date", "time"), data.table::IDateTime(visit_time))
  ][date >= (ref_date - days) & date <= ref_date,
    .(sum_feedintake = sum(feed_intake) / 1000),
    by = .(location, date)][, date := format(date, "%m-%d")][, location := as.character(location)]


  # Convert to wide format
  res <- dcast(processed_data, location ~ date, value.var = "sum_feedintake")
  numeric_cols <- setdiff(names(res), "location")
  res[, all_feedintake := round(rowSums(.SD, na.rm = TRUE), 2), .SDcols = numeric_cols]
  res[, (numeric_cols) := lapply(.SD, function(x) round(x, digits = 2)), .SDcols = numeric_cols][]
  # Return processed data table
  return(res)
}
```
  
```{r example-all_feedintake}
# Load CSV data
data <- data.table::fread("C:/Users/Dell/Documents/projects/pptsdm_data/ppt_monitor_test_data.csv")
print(all_feedintake(data = data, days = 5))
```
  
# number_location
    
```{r function-number_location}
#' Count Responders by Location and Date
#'
#' This function processes a dataset to calculate the number of responders per specified 'house'
#' (a subset of the 'location' field) for each date within a specified number of days from a reference date.
#' The data is first processed to ensure the 'date' column is in Date format, 'house' is extracted from
#' 'location', and duplicates are removed. The output is provided in wide format with dates as rows
#' and houses as columns.
#'
#' @param data A data table that must include the columns: 'location', 'date', 'responder'.
#'             The function expects that 'date' can be converted into Date format.
#' @param house_width An integer specifying the number of characters from the start of the 'location'
#'                    field to define a 'house'.
#' @param days An integer specifying the number of days to include in the analysis back from the
#'             reference date.
#' @param ref_date A Date object used as the reference date for filtering data; defaults to the current
#'                 system date.
#'
#' @return Returns a data table in wide format where each row represents a date and each column a house.
#'         Each entry is the count of responders for that house and date.
#' 
#' @export
#' 
#' @note This function depends on another function 'process data' to first ensure the data is in the correct
#'       format. It is crucial that the input data and the 'process_data' function are properly configured
#'       to handle the data correctly.
number_location <- function(data, house_width, days, ref_date = Sys.Date()) {
  house <- location <- . <- responder <- NULL
  # Ensure date column is in Date format
  data <- process_data(data)

  # Step 1: Create 'house' column by extracting specified characters from 'location'
  # Step 2: Filter data within the last 'days' from 'ref_date'
  # Step 3: Remove duplicate rows based on 'house', 'date', 'responder'
  # Consolidate these steps to streamline operations
  data <- data[, house := substr(location, 1, house_width)
  ][date >= (ref_date - days) & date <= ref_date
  ][, date := format(date, "%m-%d")][, .(house, date, responder)
  ][, .N, by = .(house, date, responder)]

  # Step 4: Group by 'house' and 'date' to count the number of responders
  summary_data <- data[, .(count = .N), by = .(house, date)]

  # Step 5: Reshape the data into wide format using 'dcast'
  # 'date' as rows, 'house' as columns, and 'count' as values
  result_data <- dcast(summary_data, date ~ house, value.var = "count")

  return(result_data)
}
```
  
```{r example-number_location}
# Load CSV data
data <- data.table::fread("C:/Users/Dell/Documents/projects/pptsdm_data/ppt_monitor_test_data.csv")
print(number_location(data = data, house_width = "1", days = 5))
```
  
# house_feedintake
    
```{r function-house_feedintake}
#' Calculate Feed Intake by House and Date
#'
#' This function calculates the total feed intake for each 'house' (a subset of 'location') over a specified
#' date range, and presents the data in a wide format. The function preprocesses the data to ensure proper 
#' formatting and then groups by 'house' and 'date' to summarize feed intake, which is then converted to kilograms.
#'
#' @param data A data table that must include at least 'location', 'date', and 'feed_intake' columns.
#'             It is expected that the data is already in a suitable format for processing.
#' @param house_width An integer specifying the number of characters from the start of the 'location'
#'                    field to define a 'house'.
#' @param days An integer specifying the number of days to include in the analysis up to and including
#'             the reference date.
#' @param ref_date A Date object used as the reference date for filtering data; defaults to the current
#'                 system date.
#'
#' @return Returns a data table in wide format where each row represents a date and each column a house.
#'         The entries are the summed feed intake for each house on each date, in kilograms. An additional
#'         column 'all_feedintake' shows the total feed intake across all houses for each date.
#' 
#' @export
#' 
#' @note This function depends on another function 'process_data' to preprocess the data. Errors may occur if
#'       the input data or the 'process_data' function are not correctly configured to handle the data.
house_feedintake <- function(data, house_width, days, ref_date = Sys.Date()) {
  house <- location <- . <- feed_intake <- NULL

  data <- process_data(data)

  # Step 1: Create 'house' from the first characters of 'location' and filter by date
  data[, house := substr(location, 1, house_width)]
  filtered_data <- data[date >= (ref_date - days) & date <= ref_date][, date := format(date, "%m-%d")]

  # Step 2: Calculate sum of 'feed_intake' per 'house' and 'date', dividing by 1000
  summarized_data <- filtered_data[, .(sum_feed_intake = sum(feed_intake) / 1000), by = .(house, date)]

  # Step 3: Remove duplicates if any exist
  unique_data <- unique(summarized_data, by = c("house", "date"))

  # Step 4: Reshape data to wide format
  wide_data <- dcast(unique_data, date ~ house, value.var = "sum_feed_intake")

  # Step 5: Calculate total feed intake across all houses for each date
  numeric_cols <- setdiff(names(wide_data), "date")
  wide_data[, all_feedintake := rowSums(.SD, na.rm = TRUE), .SDcols = numeric_cols]

  # Round numeric columns to two decimal places
  numeric_cols <- setdiff(names(wide_data), "date")
  wide_data[, (numeric_cols) := lapply(.SD, round, digits = 2), .SDcols = numeric_cols][]

  return(wide_data)
}
```
  
```{r example-house_feedintake}
# Load CSV data
data <- data.table::fread("C:/Users/Dell/Documents/projects/pptsdm_data/ppt_monitor_test_data.csv")
print(house_feedintake(data = data, house_width = "1", days = 5))
```
  
    
```{r function-merge_data}
#' Merge Data and Calculate Mean Feed Intake
#'
#' This function merges two datasets based on the 'date' column and calculates the mean feed intake
#' for each common house present in both datasets. It also handles the renaming of overlapping columns
#' and rounds numerical values for better presentation.
#'
#' @param feedintake A data frame containing feed intake data with a 'date' column.
#' @param n A data frame containing count data with a 'date' column.
#'
#' @return Returns a merged data frame with all rows included from both input frames. Suffixes "_feed" 
#'         and "_n" are added to overlapping column names from the 'feedintake' and 'n' datasets respectively. 
#'         The function also adds new columns representing the mean feed intake per common house.
#' 
#' @note It is crucial to ensure that both data frames include a 'date' column for the merge operation.
#'       The function assumes that all numeric values need to be rounded to two decimal places.
merge_data <- function(feedintake, n) {
  . <- NULL
  # Merge hh2 and hh by "date", including all rows and adding suffixes to overlapping column names
  res3 <- merge(feedintake, n, by = "date", all = TRUE, suffixes = c("_feed", "_n"))

  # Extract column names exclusive to hh2 (excluding "date" and "sum_feed_intake")
  cols_feed <- names(feedintake)[!names(n) %in% c("date", "all_feedintake")]

  # Extract column names exclusive to hh (excluding "date")
  cols_n <- names(n)[!names(n) %in% c("date")]

  # Identify common columns between hh2 and hh for calculating mean feed intake
  intersect_house <- intersect(cols_feed, cols_n)

  # Append suffixes to column names for the merged table
  cols_feed_name <- paste0(intersect_house, "_feed")
  cols_n_name <- paste0(intersect_house, "_n")

  # Calculate mean feed intake for each common house and assign it to new columns
  res3[, paste0(intersect_house, "_mean_feed") := Map(function(feed, n) res3[[feed]] / res3[[n]],
                                                      cols_feed_name, cols_n_name)]

  # Identify and round double columns to three decimal places
  double_cols <- names(res3)[sapply(res3, is.double)]
  res3[, (double_cols) := lapply(.SD, function(x) round(x, digits = 2)), .SDcols = double_cols]

  return(res3)
}
```
  
# hour_stat
    
```{r function-hour_stat}
#' Hourly Statistics Computation
#'
#' The `hour_stat` function computes hourly statistics for given data,
#' including the number of visits, total feed time, and total feed intake,
#' grouped by location and hour. This function is designed for processing
#' time-series data related to feeding or visit behaviors.
#'
#' @param data data.table, required.
#'   The input data table which must contain the following columns:
#'   `visit_time` (POSIXct), `location` (character), `duration` (numeric, in minutes),
#'   and `feed_intake` (numeric, in grams).
#' @param target_date Date, optional.
#'   The specific date for which the data should be filtered. Defaults to 35 days prior
#'   to today (using `Sys.Date() - 35`).
#'
#' @return A list of data.tables containing hourly statistics:
#' \itemize{
#'   \item `visit_n`: Number of visits per location per hour.
#'   \item `feed_time`: Total feed time in hours per location per hour.
#'   \item `feed_intake`: Total feed intake in kilograms per location per hour.
#' }
#' 
#' @export
#' 
#' @note
#' The function requires that the `data` table contains specific columns named
#' `visit_time`, `location`, `duration`, and `feed_intake`. The `visit_time` should
#' be in POSIXct format for accurate processing. 
hour_stat <- function(data, target_date = Sys.Date() - 1) {
  visit_time <- location <- duration <- . <- n <- feedintake <- NULL
  # Validate if required columns exist
  required_cols <- c("visit_time", "location", "duration", "feed_intake")
  if (!all(required_cols %in% names(data))) {
    stop("Data is missing one or more of the required columns: visit_time, location, duration, feed_intake")
  }

  # Ensure the target_date is a Date object
  if (!inherits(target_date, "Date")) {
    target_date <- as.Date(target_date)
  }

  # Process and filter data for the target date
  processed_data <- data[, `:=`(date = as.Date(visit_time),
                                hour = data.table::hour(visit_time))
  ][date == target_date][!is.na(location)][, `:=`(n = .N, feed_time = sum(duration)/60, feedintake = sum(feed_intake)/1000), by = .(location, hour)
  ][, .(location, date, hour, n, feed_time, feedintake)][, unique(.SD)]

  # Round numeric columns to two decimal places
  numeric_cols <- grep("feed_time|feedintake", names(processed_data), value = TRUE)
  data_temp <- processed_data[, (numeric_cols) := lapply(.SD, function(x) round(x, digits = 2)), .SDcols = numeric_cols]

  # Cast data for visualization or further analysis
  visit_n <- data_temp[, dcast(.SD, location ~ hour, value.var = "n")]
  feed_time <- data_temp[, dcast(.SD, location ~ hour, value.var = "feed_time")]
  feed_intake <- data_temp[, dcast(.SD, location ~ hour, value.var = "feedintake")]

  return(list(visit_n = visit_n, feed_time = feed_time, feed_intake = feed_intake))
}
```
  
```{r example-hour_stat}
# Load CSV data
data <- data.table::fread("C:/Users/Dell/Documents/projects/pptsdm_data/ppt_monitor_test_data.csv")
print(hour_stat(data = data))
```
  
  
# table_monitor

\`table_monitor\` serves as a complement to two other monitoring functions. It primarily aims to analyze and monitor, based on the CSV data from the past n days of measurement stations, the daily counts of missing records, extreme weight recordings, total feeding time at each measurement station, total feeding amount per pen per day, average weight per pen per day, and feed intake information in each hour.

```{r function-table_monitor}
#' Table Monitor Function
#'
#' The `table_monitor` function processes input data and generates an Excel workbook 
#' with various monitoring statistics and conditional formatting. It is designed for 
#' analyzing and visualizing data related to house monitoring, feed intake, and other 
#' relevant metrics over a specified period.
#'
#' @param data data.frame, required.
#'   The input data frame containing the data to be processed.
#' @param house_width character, optional.
#'   A character string representing the house width. Default is "1".
#' @param days integer, optional.
#'   An integer representing the number of days for the analysis. Default is 7.
#' @param ref_date Date, optional.
#'   A Date object representing the reference date. Default is the current date 
#'   (`Sys.Date()`).
#' @param save_path character, required.
#'   A character string representing the path where the Excel file will be saved. 
#'   Must be a non-empty string ending with ".xlsx".
#'
#' @return A list containing various monitoring statistics:
#' \itemize{
#'   \item `responder_na`: Data frame with responder NA statistics.
#'   \item `extreme_weight`: Data frame with extreme weight outlier statistics.
#'   \item `feed_time_n`: Data frame with feed time statistics.
#'   \item `low_feedintake`: Data frame with low feed intake statistics.
#'   \item `all_feedintake`: Data frame with all feed intake statistics.
#'   \item `mean_feedintake`: Data frame with mean feed intake statistics.
#'   \item `house_weight`: Data frame with house weight statistics.
#'   \item `visit_n_hour`: Data frame with the number of visits per hour.
#'   \item `feed_time_hour`: Data frame with feed time per hour.
#'   \item `feed_intake_hour`: Data frame with feed intake per hour.
#' }
#'
#' @import data.table
#' @import openxlsx
#' @importFrom stats "quantile" "time"
#' @export
#' 
#' @note
#' The `save_path` parameter must be a valid path ending with ".xlsx". Ensure that 
#' the `data` parameter is a data frame and contains the necessary columns for 
#' processing.
table_monitor <- function(data, house_width = "1", days = 5, ref_date = Sys.Date(), save_path) {
  
  # Check parameters
  if (missing(data) || is.null(data)) stop("data cannot be NULL")
  if (is.data.frame(data)) data <- data.table::as.data.table(data.table::copy(data))
  if (is.null(house_width)) stop("house_width cannot be NULL")
  if (is.numeric(house_width)) house_width <- as.character(house_width)
  if (is.null(days)) stop("days cannot be NULL")
  if (is.character(days)) days <- as.integer(days)
  
  # Check ref_date parameter
  if (!lubridate::is.Date(ref_date)) {
    stop("ref_date must be a valid date")
  }
  
  # Check save_path parameter
  if (missing(save_path) || is.null(save_path) || !is.character(save_path)) {
    stop("save_path must be a non-empty string ending with '.xlsx'")
  }
  
  # Processing data
  data <- unique(data)
  with_responder_na <- responder_na(data, days, ref_date)
  with_extreme_time_n <- extreme_time_n(data, days, ref_date)
  with_process_data <- process_data(data)
  with_low_feedintake <- low_feedintake(data, days, ref_date)
  with_all_feedintake <- all_feedintake(data, days, ref_date)
  with_number_location <- number_location(data, house_width, days, ref_date)
  with_house_feedintake <- house_feedintake(data, house_width, days, ref_date)
  with_merge_data <- merge_data(with_house_feedintake, with_number_location)
  with_house_weight <- house_weight(data, house_width, days, ref_date)
  with_hour_stat <- hour_stat(data, target_date = ref_date - 1)
  
  all_monitor <- list(
    responder_na = with_responder_na,
    extreme_weight = with_extreme_time_n$weight_outlier,
    feed_time_n = with_extreme_time_n$feedintake,
    low_feedintake = with_low_feedintake,
    all_feedintake = with_all_feedintake,
    mean_feedintake = with_merge_data,
    house_weight = with_house_weight,
    visit_n_hour = with_hour_stat$visit_n,
    feed_time_hour = with_hour_stat$feed_time,
    feed_intake_hour = with_hour_stat$feed_intake
  )
  
  # Create a new xlsx file
  wb <- openxlsx::createWorkbook()
  
  # Loop through each sheet name in the all_monitor list
  for (sheet_name in names(all_monitor)) {
    data <- all_monitor[[sheet_name]]  # Extract the data for the current sheet
    
    # Add a worksheet to the workbook
    openxlsx::addWorksheet(wb, sheet_name)
    
    # Write the data to the worksheet
    openxlsx::writeData(wb, sheet = sheet_name, data, rowNames = FALSE, colNames = TRUE)
    
    # Define different rules for different data frames if needed
    if (sheet_name == "responder_na") {
      # Calculate the 25th, 50th, and 75th percentiles for the column sum_na
      q1 <- quantile(data$total_nas, 0.25, na.rm = TRUE)
      q2 <- quantile(data$total_nas, 0.5, na.rm = TRUE)
      q3 <- quantile(data$total_nas, 0.75, na.rm = TRUE)
      
      # Add conditional formatting based on the calculated percentiles
      openxlsx::conditionalFormatting(wb, sheet = sheet_name, cols = ncol(data), rows = 2:(nrow(data) + 1),
                                      rule = c(q1, q2, q3),
                                      type = "colorScale",
                                      style = c("#63BE7B", "#FFEB84", "#F8696B"))
    } else if (sheet_name == "extreme_weight") {
      # Calculate the 25th, 50th, and 75th percentiles for the entire data frame
      data_values <- unlist(data[, 2:ncol(data)], use.names = FALSE)
      q1 <- quantile(data_values, 0.25, na.rm = TRUE)
      q2 <- quantile(data_values, 0.5, na.rm = TRUE)  # Median
      q3 <- quantile(data_values, 0.75, na.rm = TRUE)
      
      # Add conditional formatting based on the calculated percentiles
      openxlsx::conditionalFormatting(wb, sheet = sheet_name, cols = 2:(ncol(data) + 1), rows = 2:(nrow(data) + 1),
                                      rule = c(q1, q2, q3),
                                      type = "colorScale",
                                      style = c("#63BE7B", "#FFEB84", "#F8696B"))
    } else if (sheet_name == "all_feedintake") {
      # Calculate the 25th, 50th, and 75th percentiles for the column sum_feed_intake
      q1 <- quantile(data$all_feedintake, 0.25, na.rm = TRUE)
      q2 <- quantile(data$all_feedintake, 0.5, na.rm = TRUE)
      q3 <- quantile(data$all_feedintake, 0.75, na.rm = TRUE)
      
      # Add conditional formatting based on the calculated percentiles
      openxlsx::conditionalFormatting(wb, sheet = sheet_name, cols = ncol(data), rows = 2:(nrow(data) + 1),
                                      rule = c(q1, q2, q3),
                                      type = "colorScale",
                                      style = c("#F8696B", "#FFEB84", "#63BE7B"))
    } else if (sheet_name == "mean_feedintake") {
      # Select columns with names containing "mean"
      mean_cols <- grep("mean_feed", colnames(data), value = TRUE)
      n_cols <- length(mean_cols)
      
      # Calculate the 25th, 50th, and 75th percentiles for the entire data frame
      data_values <- unlist(data[, ..mean_cols], use.names = FALSE)
      q1 <- quantile(data_values, 0.25, na.rm = TRUE)
      q2 <- quantile(data_values, 0.5, na.rm = TRUE)  # Median
      q3 <- quantile(data_values, 0.75, na.rm = TRUE)
      
      # Add conditional formatting based on the calculated percentiles
      openxlsx::conditionalFormatting(wb, sheet = sheet_name, cols = (ncol(data) - n_cols + 1):ncol(data), rows = 2:(nrow(data) + 1),
                                      rule = c(q1, q2, q3),
                                      type = "colorScale",
                                      style = c("#F8696B", "#FFEB84", "#63BE7B"))
    } else if (sheet_name == "house_weight") {
      # Calculate the 25th, 50th, and 75th percentiles for the entire data frame
      data_values <- unlist(data[, 2:ncol(data)], use.names = FALSE)
      q1 <- quantile(data_values, 0.25, na.rm = TRUE)
      q2 <- quantile(data_values, 0.5, na.rm = TRUE)  # Median
      q3 <- quantile(data_values, 0.75, na.rm = TRUE)
      
      # Add conditional formatting based on the calculated percentiles
      openxlsx::conditionalFormatting(wb, sheet = sheet_name, cols = 2:(ncol(data) + 1), rows = 2:(nrow(data) + 1),
                                      rule = c(q1, q2, q3),
                                      type = "colorScale",
                                      style = c("#F8696B", "#FFEB84", "#63BE7B"))
    } else if (sheet_name %chin% c("visit_n_hour", "feed_time_hour", "feed_intake_hour")) {
      # Calculate the 25th, 50th, and 75th percentiles for the entire data frame
      numeric_columns <- sapply(data, is.numeric)
      data_values <- unlist(data[, ..numeric_columns], use.names = FALSE)
      q1 <- quantile(data_values, 0.25, na.rm = TRUE)
      q2 <- quantile(data_values, 0.5, na.rm = TRUE)  # Median
      q3 <- quantile(data_values, 0.75, na.rm = TRUE)
      
      # Add conditional formatting based on the calculated percentiles
      openxlsx::conditionalFormatting(wb, sheet = sheet_name, cols = 2:(ncol(data) + 1), rows = 2:(nrow(data) + 1),
                                      rule = c(q1, q2, q3),
                                      type = "colorScale",
                                      style = c("#F8696B", "#FFEB84", "#63BE7B"))
    } else if (sheet_name == "low_feedintake") {
      # Calculate the 25th, 50th, and 75th percentiles for the entire data frame
      numeric_columns <- sapply(data, is.numeric)
      data_values <- unlist(data[, ..numeric_columns], use.names = FALSE)
      q1 <- quantile(data_values, 0.25, na.rm = TRUE)
      q2 <- quantile(data_values, 0.5, na.rm = TRUE)  # Median
      q3 <- quantile(data_values, 0.75, na.rm = TRUE)
      
      # Add conditional formatting based on the calculated percentiles
      openxlsx::conditionalFormatting(wb, sheet = sheet_name, cols = 3:(ncol(data) + 1), rows = 2:(nrow(data) + 1),
                                      rule = c(q1, q2, q3),
                                      type = "colorScale",
                                      style = c("#F8696B", "#FFEB84", "#63BE7B"))
    } else {
      # Select columns with names containing "sum_n"
      sum_n_cols <- grep("sum_n", colnames(data), value = TRUE)
      n_cols <- length(sum_n_cols)
      
      # Calculate the 25th, 50th, and 75th percentiles for the entire data frame
      data_values <- unlist(data[, ..sum_n_cols], use.names = FALSE)
      q1 <- quantile(data_values, 0.25, na.rm = TRUE)
      q2 <- quantile(data_values, 0.5, na.rm = TRUE)  # Median
      q3 <- quantile(data_values, 0.75, na.rm = TRUE)
      
      # Add conditional formatting based on the calculated percentiles
      openxlsx::conditionalFormatting(wb, sheet = sheet_name, cols = (ncol(data) - n_cols + 1):ncol(data), rows = 2:(nrow(data) + 1),
                                      rule = c(q1, q2, q3),
                                      type = "colorScale",
                                      style = c("#F8696B", "#FFEB84", "#63BE7B"))
    }
    
    # Create a bold style for column and row names
    header_style <- openxlsx::createStyle(textDecoration = "bold", halign = "center")
    
    # Apply the bold style to column names
    openxlsx::addStyle(wb, sheet = sheet_name, style = header_style, rows = 1, cols = 1:ncol(data), gridExpand = TRUE)
    
    # Create a center alignment style for cell contents
    center_style <- openxlsx::createStyle(halign = "center")
    
    # Apply the center alignment style to cell contents
    openxlsx::addStyle(wb, sheet = sheet_name, style = center_style, rows = 2:(nrow(data) + 1), cols = 1:ncol(data), gridExpand = TRUE)
    
    # Optionally, apply the bold style to row names
    # In this example, assume the first column contains row names
    openxlsx::addStyle(wb, sheet = sheet_name, style = header_style, rows = 1:(nrow(data) + 1), cols = 1, gridExpand = TRUE)
    
    # Auto adjust column widths
    openxlsx::setColWidths(wb, sheet = sheet_name, cols = 1:ncol(data), widths = "auto")
  }
  
  # Save the xlsx file
  openxlsx::saveWorkbook(wb, file.path(save_path, "table_monitor.xlsx"), overwrite = TRUE)
  
  return(all_monitor)
}
```

```{r examples-table_monitor}
# Load CSV data
data <- data.table::fread("C:/Users/Dell/Documents/projects/pptsdm_data/ppt_monitor_test_data.csv")
# Monitor station and data
res <- table_monitor(data = data, days = 5, save_path = "C:/Users/Dell/Downloads/test")
# Monitor the number of times 'na' appears in the last n days
head(res$responder_na)
# Monitor the percentage of extreme weight records in the last n days
head(res$extreme_weight)
# Monitor the visiting time and frequency of pigs in the last n days
head(res$feed_time_n)
# Monitor the low feedintake over the last n days
head(res$low_feedintake)
# Monitor the total feed intake over the last n days
head(res$all_feedintake)
# Monitor the average feed intake over the last n days
head(res$mean_feedintake)
# Monitor the average weight per pen over the last n days
head(res$house_weight)
# Monitor visit time in each hour over the last 1 day.
head(res$visit_n_hour)
# Monitor feed intake time in each hour over the last 1 day.
head(res$feed_time_hour)
# Monitor feed intake in each hour over the last 1 day.
head(res$feed_intake_hour)
```

# monitor_schedule

You can use `monitor_schedule` to set when to run the `fid_monitor()`、`station_monitor()` and `table_monitor()`.

```{r function-monitor_schedule}
#' Feed intake monitor of pig performance test station
#' 
#' @param taskname A character string with the name of the task. Defaults to the filename. Should not contain any spaces
#' @param schedule Either one of 'ONCE', 'MONTHLY', 'WEEKLY', 'DAILY', 'HOURLY', 'MINUTE', 'ONLOGON', 'ONIDLE
#' @param starttime A timepoint in HH:mm format indicating when to run the script. Defaults to within 62 seconds
#' @param startdate A date that specifies the first date on which to run the task. Only applicable if schedule is of type 'MONTHLY', 'WEEKLY', 'DAILY', 'HOURLY', 'MINUTE'. Defaults to today in '%d/%m/%Y' format. Change to your locale format if needed
#' @param rscript_args Character string with further arguments passed on to Rscript
#' @param ... other parameters
#'
#' @importFrom utils "capture.output"
#'
#' @return pdf, pngs and excels
#' @export
#' 
monitor_schedule <- function(taskname, schedule, starttime, startdate, rscript_args = NULL, ...) {
  if (missing(taskname) || !is.character(taskname) || length(taskname) != 1) {
    stop("taskname must be a single character string")
  }

  if (missing(schedule) || !is.character(schedule) || length(schedule) != 1) {
    stop("schedule must be a single character string")
  }

  if (missing(starttime) || !is.character(starttime) || length(starttime) != 1) {
    stop("starttime must be a single character string")
  }

  if (missing(startdate) || !is.character(startdate) || length(startdate) != 1) {
    stop("startdate must be a single character string")
  }

  if (missing(rscript_args) || !is.list(rscript_args)) {
    stop("rscript_args must be a list of arguments")
  }

  # Save the function to a temporary script file with a shorter path
  short_temp_path <- "C:/Temp"
  dir.create(short_temp_path, showWarnings = FALSE)
  script_file <- file.path(short_temp_path, paste0(taskname, "_", sample(letters, 1), ".R"))

  my_function <- function(csv_path, begin_date, house_width, days, ref_date, ...) {
    csv_files <- list.files(csv_path, full.names = T, pattern = ".csv", recursive = T)
    csv_data <- pptsda::import_csv(csv_files, package = "data.table")
    pptsdm::fid_monitor(data = csv_data, begin_date = begin_date, station_type = "nedap", ...)
    pptsdm::station_monitor(data = csv_data, begin_date = begin_date, station_type = "nedap", ...)
    pptsdm::table_monitor(data = csv_data, house_width = house_width, days = days, ref_date = ref_date, ...)
  }

  # Save the arguments to a configuration file
  config_file <- file.path(short_temp_path, paste0("monitor_", taskname, ".txt"))
  cat("arg_list <- ", capture.output(dput(rscript_args)), file = config_file)

  write_function_to_script <- function(func, file_path, config_path) {
    func_name <- deparse(substitute(func))
    lines <- capture.output(dump(func_name, stdout()))
    lines <- c(lines, sprintf("source('%s')", config_path))
    lines <- c(lines, sprintf("do.call(%s, arg_list)", func_name))

    if (!file.exists(script_file)) {
      file.create(script_file)
    }

    con <- file(script_file, "w")
    on.exit(close(con), add = TRUE)
    writeLines(lines, con)
  }

  write_function_to_script(func = my_function, file_path = script_file, config_path = config_file)

  # Schedule the task
  taskscheduleR::taskscheduler_create(taskname = taskname,
                                      rscript = script_file,
                                      schedule = schedule,
                                      starttime = starttime,
                                      startdate = startdate,
                                      rscript_args = NULL,
                                      ...)
}
```

```{r examples-monitor_schedule}
# Set monitor task
monitor_schedule(
  taskname = "ppt_csv_monitor",
  schedule = "DAILY",
  starttime = "10:05",
  startdate = format(Sys.Date(), "%Y/%m/%d"),
  rscript_args = list(house_width = "1", 
                      days = 5,
                      ref_date = Sys.Date(),
                      begin_date = "2024-05-01", 
                      csv_path = "C:/Users/Dell/Documents/projects/pptsdm_data",
                      save_path = "C:/Users/Dell/Downloads/test"))
# Delete monitor task
taskscheduleR::taskscheduler_delete("ppt_csv_monitor")
```



That's it ! This the end of the documented story of our package. All components are there.




```{=html}
<!-- 
# Inflate your package

You're one inflate from paper to box.
Build your package from this very Rmd using `fusen::inflate()` 
-->
```
```{r development-inflate, eval=FALSE}
# Execute in the console directly
fusen::inflate(flat_file = "dev/station_pig_monitor.Rmd", check = T, vignette_name = "Basic Usage")
```

```{=html}
<!-- 
- Verify your `"DESCRIPTION"` file has been updated
- Verify your function is in `"R/"` directory
- Verify your test is in `"tests/testthat/"` directory
- Verify this Rmd appears in `"vignettes/"` directory 
-->
```
