---
title: "flat_teaching.Rmd for working package"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{=html}
<!-- 
Run this 'development' chunk

Store every call to library() that you need to run chunks line by line, as in a classical Rmd for analysis
-->
```
```{r development, include=FALSE}
utils::globalVariables(c("..mean_cols", "..numeric_columns", "..sum_n_cols")
library(data.table)
library(ggplot2)
library(tidyfst)
library(robustbase)
library(taskscheduleR)
library(patchwork)
library(lubridate)
library(openxlsx)
```

```{=html}
<!--
# Description of your package

This will fill the description of your package.
Fill and run the content of this chunk, before anything else. 

Note: when you will use other flat templates, this part will be in a separate file. Do not be surprised!
-->
```
```{r description, eval=FALSE}
# Describe your package
fusen::fill_description(
  pkg = here::here(),
  fields = list(
    Title = "Breeding Tools",
    Description = "An R package that can monitor the csv data and stations from the Nedap in pig farm.",
     `Authors@R` = c(
      person("Guo Meng", email = "tony2015116@163.com", role = c("aut", "cre")),
      person(given = "Guo Meng", role = "cph")
    )
  ),
  overwrite=T
)
# Define License with use_*_license()
usethis::use_mit_license("Guo Meng")
```

# fid_monitor

You can use `fid_monitor()` to monitor the feed intake of each pig in the nedap or fire pig performance test station as well as the total feed intake of all pigs.

```{r function-fid_monitor}
#' Feed intake monitor of pig performance test station
#' 
#' @param data A data frame or data table containing the nedap or fire pig performance test data to be processed. Columns must include 'visit_time', 'location', 'responder', 'feed_intake'.
#' 
#' @param begin_date An optional Date object or character string specifying the beginning date for the data to be processed. If not provided, all dates in the data will be considered.
#' 
#' @param station_type A character string specifying the type of station. This must be either 'nedap' or 'fire'.
#' @param save_path A character string specifying the path where the output PDF will be saved.
#' 
#' @importFrom data.table ":="
#' @importFrom data.table ".SD"
#' 
#' @return This function does not return a value. It saves a PDF file to the specified path.
#' @export
#' 
fid_monitor <- function(data, begin_date=NULL, station_type, save_path) {
  if (missing(data)) stop("Please provide 'data' argument.")
  if (missing(station_type)) stop("Please provide 'station_type' argument.")
  if (missing(save_path)) stop("Please provide 'save_path' argument.")
  # Argument checks
  if (!is.data.frame(data) && !data.table::is.data.table(data)) {
    stop("The 'data' argument must be a data.frame or a data.table.")
  }
  
  if (!is.null(begin_date)) {
    if (!inherits(begin_date, "Date") && !is.character(begin_date)) {
      stop("Error: 'begin_date' argument must be a Date object or character string.")
    }
    
    if (is.character(begin_date)) {
      begin_date <- as.Date(begin_date)
    }
  }
  
  if (!is.character(station_type) || !(station_type %in% c("nedap", "fire"))) {
    stop("The 'station_type' argument must be either 'nedap' or 'fire'.")
  }
  
  if (!is.character(save_path) || !dir.exists(save_path)) {
    stop("The 'save_path' argument must be a valid directory path.")
  }
  
  visit_time <- . <- location <- responder <- feed_intake <- all_feed_a_station_one_day <- total_intake <- percent_intake <- Date <- Consumed <- total_intake <- percent_intake <- ndt <- plot1 <- plot2 <- NULL
  
  if (!data.table::is.data.table(data)) {
    data <- data.table::setDT(data)
  }
  prepare_nedap_data <- function(data, begin_date = NULL) {
    temp1 <- unique(data)[,`:=`(c("date", "time"),data.table::tstrsplit(visit_time," ", fixed = TRUE))][, `:=`(c("date"), lubridate::ymd(date))][,!c("visit_time", "time")]
    temp2 <- temp1[, keyby = .(location, responder, date),.(total_intake = round(sum(feed_intake) / 1000, digits = 4))][,`:=`(all_feed_a_station_one_day, sum(total_intake)), by = .(location, date)][, `:=`(percent_intake, total_intake / all_feed_a_station_one_day)]
    to_factor = c("location", "responder")
    temp2[, `:=`((to_factor), purrr::map(.SD, as.factor)), .SDcols = to_factor]
    if (!is.null(begin_date)) {
      temp2 <- temp2[date >= begin_date]
    }
    return(temp2)
  }
  
  prepare_fire_data <- function(data, begin_date = NULL) {
    temp1 <- unique(data)[,`:=`(Date, lubridate::ymd(Date))]
    data.table::setnames(temp1, 1:3, c("location", "responder","date"))
    temp2 <- temp1[, keyby = .(location, responder, date),.(total_intake = round(sum(Consumed), digits = 4))][,`:=`(all_feed_a_station_one_day, sum(total_intake)),by = .(location, date)][, `:=`(percent_intake, total_intake / all_feed_a_station_one_day)]
    to_factor = c("location", "responder")
    temp2[, `:=`((to_factor), purrr::map(.SD, as.factor)),.SDcols = to_factor]
    if (!is.null(begin_date)) {
      temp2 <- temp2[date >= begin_date]
    }
    return(temp2)
  }
  
  # Prepare data based on station_type
  if (station_type == "nedap") {
    prepared_data <- prepare_nedap_data(data, begin_date = begin_date)
  } else if (station_type == "fire") {
    prepared_data <- prepare_fire_data(data, begin_date = begin_date)
  } else {
    stop("Invalid station_type. Supported types are 'nedap' and 'fire'.")
  }
  
  # Create the plots
  colors = c("#a6cee3", "#1f78b4", "#b2df8a", "#33a02c", "#fb9a99", "#e31a1c", "#fdbf6f", "#ff7f00", "#cab2d6", "#6a3d9a", "#b15928", "#8dd3c7", "#d9d9d9", "#80b1d3", "#00AFBB", "#01665e", "#003c30", "blue", "pink", "yellow", "red", "green", "#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#00AFBB", "#E7B800", "#FC4E07", "#1B9E77", "#D95F02", "#7570B3", "#E7298A", "#66A61E", "#E6AB02", "#A6761D", "#666666", "purple")
  #colors = as.character(palette.colors(n = 36, "Polychrome 36"))
  
  create_plots <- function(data, ...) {
    data[, list(list(.SD)), by = location
    ][, `:=`("plot1", purrr::map2(.SD[[1]], location, ~ ggplot2::ggplot(
      data = .x,
      ggplot2::aes(y = percent_intake,
                   x = date, fill = responder)
    ) + ggplot2::theme_bw() +
      ggplot2::geom_col(
        width = 0.8,
        na.rm = F,
        show.legend = T
      ) +
      ggplot2::ggtitle(.y) + ggplot2::scale_y_continuous(
        labels = scales::percent,
        limits = c(0, 1),
        breaks = seq(0, 1, 0.1)
      ) + ggplot2::theme(
        plot.title = ggplot2::element_text(
          color = "black",
          hjust = 0.5,
          size = 20
        ),
        axis.title.x = ggplot2::element_blank(),
        axis.title.y = ggplot2::element_text(size = 15),
        axis.text.x = ggplot2::element_text(size = 8, angle = 270),
        axis.text.y = ggplot2::element_text(size = 8),
        legend.title = ggplot2::element_text(size = 8),
        legend.text = ggplot2::element_text(size = 8),
        legend.position = "top",
        legend.margin = ggplot2::margin(10, 10, 10, 10),  # Adjust the legend margin
        legend.box.margin = ggplot2::margin(0, 0, 30, 0)  # Extra space at the bottom for the legend
      ) +
      ggplot2::guides(
        shape = ggplot2::guide_legend(override.aes = list(size = 7)),
        color = ggplot2::guide_legend(override.aes = list(size = 7))
      ) +
      ggplot2::ggtitle(paste0("location:", .y)) + ggplot2::scale_x_date(
        date_breaks = "1 day",
        date_labels = "%m-%d",
        date_minor_breaks = "1 day"
      ) +
      ggplot2::scale_fill_manual(
        na.value = "black",
        values = colors
      ), ...)), .SDcols = "V1"
    ][, `:=`("plot2", purrr::map2(.SD[[1]], location, ~ ggplot2::ggplot(data = .x,
                                                                        ggplot2::aes(
                                                                          y = total_intake, x = date, fill = responder
                                                                        )) +
                                    ggplot2::theme_bw() + ggplot2::geom_col(
                                      width = 0.8,
                                      na.rm = F,
                                      show.legend = F
                                    ) + ggplot2::theme(
                                      plot.title = ggplot2::element_text(
                                        color = "black",
                                        hjust = 0.5,
                                        size = 20
                                      ),
                                      axis.title.x = ggplot2::element_text(size = 15),
                                      axis.title.y = ggplot2::element_text(size = 15),
                                      axis.text.x = ggplot2::element_text(size = 8, angle = 270),
                                      axis.text.y = ggplot2::element_text(size = 8),
                                      legend.position = "none"
                                    ) +
                                    ggplot2::guides(
                                      shape = ggplot2::guide_legend(override.aes = list(size = 7)),
                                      color = ggplot2::guide_legend(override.aes = list(size = 7))
                                    ) +
                                    ggplot2::scale_x_date(
                                      date_breaks = "1 day",
                                      date_labels = "%m-%d",
                                      date_minor_breaks = "1 day"
                                    ) + ggplot2::scale_fill_manual(
                                      na.value = "black",
                                      values = colors
                                    ), ...)), .SDcols = "V1"][]
  }
  
  # Combine and save the plots
  save_combined_plots <- function(path_out, ...) {
    temp_plot <- create_plots(data = prepared_data, value = colors)
    
    # Calculate the number of unique locations
    num_locations <- length(unique(temp_plot$location))
    
    # Calculate the date range
    date_range <- range(prepared_data$date)
    num_days <- as.numeric(difftime(date_range[2], date_range[1], units = "days")) + 1
    
    # Set the minimum days for width calculation to 30
    num_days_for_width <- max(num_days, 50)
    
    # Calculate the PDF dimensions based on the number of locations and date range
    width_per_day <- 0.3 # 0.5 cm per day, you can adjust this value based on your preferences
    pdf_width <- num_days_for_width * width_per_day * 2 # Adjust width according to the date range and number of columns
    
    height_per_location <- 30 # 30 cm per location, you can adjust this value based on your preferences
    pdf_height <- height_per_location * ceiling(num_locations / 2) # Adjust height according to the number of locations and number of rows
    
    temp4 <- temp_plot[, `:=`("finals", purrr::pmap(.(.SD[[1]], .SD[[2]]), function(x, y) patchwork::wrap_plots(x, y, ncol = 1))), .SDcols = c("plot1","plot2")][]
    temp5 <- patchwork::wrap_plots(temp4$finals, ncol = 2)
    
    ggplot2::ggsave(
      file.path(save_path, "feed_intake_monitor.pdf"),
      temp5,
      width = pdf_width,
      height = pdf_height,
      units = "cm",
      dpi = "retina",
      limitsize = FALSE,
      ...
    )
  }
  save_combined_plots(save_path)
}
```

```{r examples-fid_monitor, warning=FALSE}
# Load CSV data
data <- data.table::fread("C:/Users/Dell/Documents/projects/pptsdm_data/ppt_monitor_test_data.csv")
# Feed intake monitor
fid_monitor(data = data, station_type = "nedap", save_path = "C:/Users/Dell/Downloads/test")
```

```{=html}
<!-- 
This first section shows:

- the three parts necessary for a package: 'function', 'examples' and 'tests'.  
  + Note that the three following chunks have names accordingly.

-->
```
```{=html}
<!--
Here is an example on how to use the function.
This should be a reproducible and working example
-->
```
# station_monitor

You can use function of `station_monitor()` to monitor pig performance test data from nedap and fire pig performance test stations. This function can monitor the changes in the number of pigs, total visitation time of pigs, total number of visits, changes in total feed intake, and also monitor the changes in pig weights for a specific pig performance test station.

```{r function-station_monitor}
#' Feed intake monitor of pig performance test station
#' 
#' @param data A data frame or data table. This is the data to be processed. It should have specific columns depending on the station type.
#' @param begin_date An optional parameter. If provided, only data from this date onwards will be considered. It can be a Date object or a character string in the form 'yyyy-mm-dd'. Default is NULL, which means all dates in the data will be considered.
#' @param station_type A character string specifying the type of station. This must be either 'nedap' or 'fire'.
#' @param save_path A character string specifying the path where the output PNG files will be saved.
#' 
#' @importFrom data.table ":=" "CJ" ".SD"
#' 
#' @return This function does not return a value. It saves PNG files to the specified path.
#' @export
#' 
station_monitor <- function (data, begin_date=NULL, station_type, save_path)
{
  if (missing(data)) stop("Please provide 'data' argument.")
  if (missing(station_type)) stop("Please provide 'station_type' argument.")
  if (missing(save_path)) stop("Please provide 'save_path' argument.")
  # Argument checks
  if (!is.data.frame(data) && !data.table::is.data.table(data)) {
    stop("The 'data' argument must be a data.frame or a data.table.")
  }
  
  if (!is.null(begin_date)) {
    if (!inherits(begin_date, "Date") && !is.character(begin_date)) {
      stop("Error: 'begin_date' argument must be a Date object or character string.")
    }
    
    if (is.character(begin_date)) {
      begin_date <- as.Date(begin_date)
    }
  }
  
  if (!is.character(station_type) || !(station_type %in% c("nedap", "fire"))) {
    stop("The 'station_type' argument must be either 'nedap' or 'fire'.")
  }
  
  if (!is.character(save_path) || !dir.exists(save_path)) {
    stop("The 'save_path' argument must be a valid directory path.")
  }
  visit_time <-
    responder <-
    . <-
    location <-
    animal_number <-
    duration <-
    feed_intake <-
    Entry <-
    Exit <- Consumed <- weight <- ndt <- . <- items <- plot1 <- .N <- V1 <- hour <- plot_name <- NULL
  
  if (!data.table::is.data.table(data)) {
    data <- data.table::setDT(data)
  }
  # Prepare data based on station_type
  prepare_nedap_data <- function(data, begin_date = NULL) {
    temp1 <-
      unique(data)[, `:=`(c("date", "time"),data.table::tstrsplit(visit_time," ", fixed = TRUE))
      ][, `:=`(c("date"), data.table::as.IDate(date))
      ][,!c("visit_time", "time")]
    if (!is.null(begin_date)) {
      temp1 <- temp1[date >= begin_date]
    }
    temp2 <- unique(temp1, by = c("location", "responder", "date"))[!is.na(responder)
    ][, keyby = .(location,date), .(animal_number = .N)]
    temp3 <- temp1[!is.na(animal_number)
    ][, keyby = .(location,date), .(`total_intake_duration(min)` = round(sum(duration) / 60,digits = 4),total_intake = round(sum(feed_intake) / 1000,digits = 4),visit_number = .N)]
    temp5 <- merge(temp2, temp3, all.x = TRUE)
    list(temp5 = temp5, temp1 = temp1)
  }
  
  prepare_fire_data <- function(data, begin_date = NULL) {
    temp1 <- unique(data)[, `:=`(Entry,do.call(paste, c(.SD, sep = " "))), .SDcol = c("Date","Entry")
    ][, `:=`(Exit, do.call(paste, c(.SD, sep = " "))),.SDcol = c("Date", "Exit")
    ][, `:=`(c("Entry", "Exit"),lapply(.SD, lubridate::ymd_hms)), .SDcol = c("Entry","Exit")
    ][, `:=`(duration,data.table::fifelse(Exit -Entry < 0 & lubridate::hour(Exit) == 0,Exit - Entry + lubridate::ddays(1), Exit - Entry))]
    data.table::setnames(temp1,
                         c(1:3, 9),
                         c("location",
                           "responder", "date", "weight"))
    temp1 <- unique(temp1)[, `:=`(date, lubridate::ymd(date))]
    if (!is.null(begin_date)) {
      temp1 <- temp1[date >= begin_date]
    }
    temp2 <- unique(temp1, by = c("location", "responder","date"))[!is.na(responder)
    ][, keyby = .(location, date), .(animal_number = .N)]
    temp3 <- temp1[!is.na(responder)
    ][, `:=`(duration, as.numeric(duration))
    ][,keyby = .(location, date), .(`total_intake_duration(min)` = round(sum(duration) / 60,digits = 4), total_intake = round(sum(Consumed),digits = 4),visit_number = .N)]
    temp5 <- merge(temp2, temp3, all.x = TRUE)
    list(temp5 = temp5, temp1 = temp1)
  }
  
  
  # Prepare data based on station_type
  if (station_type == "nedap") {
    prepared_data_list <- prepare_nedap_data(data, begin_date = begin_date)
    prepared_data <- prepared_data_list$temp5
    temp1 <- prepared_data_list$temp1
  } else if (station_type == "fire") {
    prepared_data_list <- prepare_fire_data(data, begin_date = begin_date)
    prepared_data <- prepared_data_list$temp5
    temp1 <- prepared_data_list$temp1
  } else {
    stop("Invalid station_type. Supported types are 'nedap' and 'fire'.")
  }
  
  # The rest of the code remains the same.
  # 将第3到第6列的类型转换为numeric
  prepared_data[, (3:6) := lapply(.SD, as.numeric), .SDcols = 3:6]
  
  # 使用melt函数
  temp6 = data.table::melt(prepared_data,
                           id.vars = c("location", "date"),
                           measure.vars = 3:6,
                           variable.name = "items",
                           value.name = "values")
  
  temp6_2 <- temp1[!is.na(location), .(location, date, weight)
  ][, list(list(.SD)), by = location # nest by location
  ][, `:=`("V1", purrr::map(.SD[[1]], function(data) {data[data.table::CJ(date = tidyr::full_seq(date, 1)), on = .(date)
  ][data.table::CJ(date = date, unique = TRUE), on = .(date)]})), .SDcols = "V1"
  ][, `:=`("plot1", purrr::map2(.SD[[1]], location, ~ggplot2::ggplot(data = .x, ggplot2::aes(x = date, y = weight, group = date)) +
                                  ggplot2::geom_boxplot(outlier.color = "red") +
                                  ggplot2::scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
                                  cowplot::background_grid(minor = "none") +
                                  ggplot2::scale_x_date(date_breaks = "1 day", date_labels = "%d") +
                                  ggplot2::theme_bw() +
                                  ggplot2::theme(legend.position = "none",
                                                 axis.title.x = ggplot2::element_blank(),
                                                 axis.text.x = ggplot2::element_text(angle = -90),
                                                 axis.text = ggplot2::element_text(size = 8)))), .SDcols = "V1"
  ][ , V1 := NULL
  ][]
  
  temp7 <- temp6[, list(list(.SD)), by = location
  ][, `:=`("V1", purrr::map(.SD[[1]], function(data) {data[CJ(date = tidyr::full_seq(date, 1)), on = .(date)
  ][CJ(date = date, items = items, unique = TRUE), on = .(date, items)
  ][!is.na(items)
  ][,`:=`(items, factor(items, labels = c("N", "visit_time/min","feed_intake/kg", "visit_number")))]})), .SDcols = "V1"
  ][, `:=`("plot2", purrr::map2(.SD[[1]], location,~ggplot2::ggplot(data = .x, ggplot2::aes(x = date, y = values)) +
                                  ggplot2::geom_point(ggplot2::aes(col = items)) +
                                  ggplot2::geom_line(ggplot2::aes(col = items)) +
                                  ggplot2::theme_bw() +
                                  ggplot2::facet_grid(items ~ ., scales = "free") +
                                  ggplot2::scale_x_date(date_breaks = "1 day", date_labels = "%m") +
                                  ggplot2::scale_colour_brewer(palette = "Set1") +
                                  ggplot2::theme(strip.text.y = ggplot2::element_text(angle = 0,hjust = 0),
                                                 legend.position = "none",
                                                 axis.title = ggplot2::element_blank(),
                                                 axis.text.x = ggplot2::element_text(angle = -90),
                                                 axis.text = ggplot2::element_text(size = 8),
                                                 strip.placement = "outside",
                                                 strip.background = ggplot2::element_rect(colour = "white", fill = "white")) +
                                  ggplot2::ggtitle(paste0("Location:", .y)) +
                                  cowplot::background_grid(minor = "none"))), .SDcols = "V1"][]
  
  temp8 <- merge(temp7, temp6_2, by = "location", all.x = TRUE)
  temp9 <- temp8[, `:=`("finals", purrr::pmap(.(.SD[[1]], .SD[[2]]), function(x, y) patchwork::wrap_plots(x, y, nrow = 2))), .SDcols = c("plot2","plot1")
  ][, plot_name := paste0(location, "_stations.png")][]
  
  # 根据天数计算图片的长度和宽度
  # 使用 map() 遍历 V1 列中的所有列表，计算唯一日期数
  days <- purrr::map_dbl(temp9$V1, ~ length(unique(.x$date)))
  height <- ifelse(days <= 7, 8, ifelse(days <= 14, 8, ifelse(days <= 30, 8, 8)))
  width <- ifelse(days <= 7, 5, ifelse(days <= 14, 10, ifelse(days <= 30, 13, 16)))
  
  purrr::walk2(
    temp9$plot_name,
    temp9$finals,
    ~ ggplot2::ggsave(
      filename = file.path(save_path, .x),
      plot = .y,
      height = height,
      width = width
    )
  )
}
```

```{r examples-station_monitor}
# Load CSV data
data <- data.table::fread("C:/Users/Dell/Documents/projects/pptsdm_data/ppt_monitor_test_data.csv")
# Station monitor
station_monitor(data = data, station_type = "nedap", save_path = "C:/Users/Dell/Downloads/test")
```

# table_monitor

\`table_monitor\` serves as a complement to two other monitoring functions. It primarily aims to analyze and monitor, based on the CSV data from the past n days of measurement stations, the daily counts of missing records, extreme weight recordings, total feeding time at each measurement station, total feeding amount per pen per day, average weight per pen per day, and feed intake information in each hour.

```{r function-table_monitor}
#' Table feed intake monitor of pig performance test station
#' 
#' @param data A data frame or data table. This is the data to be processed. It should have specific columns depending on the station type.
#' @param house_width A character or numeric value specifying the width of the house number.If numeric, it will be converted to a character string. default house_width is 1.
#' @param days Days you can choose to monitor latest n days data, default days is 7.
#' @param save_path A character string specifying the path where the output PNG files will be saved.
#' 
#' @return A list of data.tables, each representing different aspects of station monitoring results. This list is also saved to an Excel file at the specified `save_path`.
#' @import data.table
#' @import openxlsx
#' @importFrom stats "quantile" "time"
#' @export
table_monitor <- function(data, house_width = "1", days = 7, save_path) {
  
  # Check parameters
  if (missing(data) || is.null(data)) stop("data cannot be NULL")
  if (is.data.frame(data)) data <- data.table::as.data.table(data.table::copy(data))
  if (is.null(house_width)) stop("house_width cannot be NULL")
  if (is.numeric(house_width)) house_width <- as.character(house_width)
  if (is.null(days)) stop("days cannot be NULL")
  if (is.character(days)) days <- as.integer(days)

  # Check save_path parameter
  if (missing(save_path) || is.null(save_path) || !is.character(save_path)) {
    stop("save_path must be a non-empty string ending with '.xlsx'")
  }

  # Processing data
  data <- unique(data)
  with_responder_na <- responder_na(data, days)
  with_extreme_time_n <- extreme_time_n(data, days)
  with_process_data <- process_data(data)
  with_low_feedintake <- low_feedintake(with_process_data, days)
  with_all_feedintake <- all_feedintake(with_process_data, days)
  with_number_location <- number_location(with_process_data, house_width, days)
  with_house_feedintake <- house_feedintake(with_process_data, house_width, days)
  with_merge_data <- merge_data(with_house_feedintake, with_number_location)
  with_house_weight <- house_weight(with_process_data, house_width, days)
  with_hour_stat <- hour_stat(data)

  all_monitor <- list(
    responder_na = with_responder_na,
    extreme_weight = with_extreme_time_n$extreme_ratio,
    feed_time_n = with_extreme_time_n$feed_time_n,
    all_feedintake = with_all_feedintake,
    mean_feedintake = with_merge_data,
    house_weight = with_house_weight,
    visit_n = with_hour_stat$visit_n,
    feed_time = with_hour_stat$feed_time,
    feed_intake = with_hour_stat$feed_intake
  )

  # Create a new xlsx file
  wb <- openxlsx::createWorkbook()

  # Loop through each sheet name in the all_monitor list
  for (sheet_name in names(all_monitor)) {
    data <- all_monitor[[sheet_name]]  # Extract the data for the current sheet

    # Add a worksheet to the workbook
    openxlsx::addWorksheet(wb, sheet_name)

    # Write the data to the worksheet
    openxlsx::writeData(wb, sheet = sheet_name, data, rowNames = FALSE, colNames = TRUE)

    # Define different rules for different data frames if needed
    if (sheet_name == "responder_na") {
      # Calculate the 25th, 50th, and 75th percentiles for the column sum_na
      q1 <- quantile(data$sum_na, 0.25, na.rm = TRUE)
      q2 <- quantile(data$sum_na, 0.5, na.rm = TRUE)
      q3 <- quantile(data$sum_na, 0.75, na.rm = TRUE)

      # Add conditional formatting based on the calculated percentiles
      openxlsx::conditionalFormatting(wb, sheet = sheet_name, cols = ncol(data), rows = 2:(nrow(data) + 1),
                            rule = c(q1, q2, q3),
                            type = "colorScale",
                            style = c("#63BE7B", "#FFEB84", "#F8696B"))
    } else if (sheet_name == "extreme_weight") {
      # Calculate the 25th, 50th, and 75th percentiles for the entire data frame
      data_values <- unlist(data[, 2:ncol(data)], use.names = FALSE)
      q1 <- quantile(data_values, 0.25, na.rm = TRUE)
      q2 <- quantile(data_values, 0.5, na.rm = TRUE)  # Median
      q3 <- quantile(data_values, 0.75, na.rm = TRUE)

      # Add conditional formatting based on the calculated percentiles
      openxlsx::conditionalFormatting(wb, sheet = sheet_name, cols = 2:(ncol(data) + 1), rows = 2:(nrow(data) + 1),
                            rule = c(q1, q2, q3),
                            type = "colorScale",
                            style = c("#63BE7B", "#FFEB84", "#F8696B"))
    } else if (sheet_name == "all_feedintake") {
      # Calculate the 25th, 50th, and 75th percentiles for the column sum_feed_intake
      q1 <- quantile(data$sum_feed_intake, 0.25, na.rm = TRUE)
      q2 <- quantile(data$sum_feed_intake, 0.5, na.rm = TRUE)
      q3 <- quantile(data$sum_feed_intake, 0.75, na.rm = TRUE)

      # Add conditional formatting based on the calculated percentiles
      openxlsx::conditionalFormatting(wb, sheet = sheet_name, cols = ncol(data), rows = 2:(nrow(data) + 1),
                            rule = c(q1, q2, q3),
                            type = "colorScale",
                            style = c("#F8696B", "#FFEB84", "#63BE7B"))
    } else if (sheet_name == "mean_feedintake") {
      # Select columns with names containing "mean"
      mean_cols <- grep("mean_feed", colnames(data), value = TRUE)
      n_cols <- length(mean_cols)

      # Calculate the 25th, 50th, and 75th percentiles for the entire data frame
      data_values <- unlist(data[, ..mean_cols], use.names = FALSE)
      q1 <- quantile(data_values, 0.25, na.rm = TRUE)
      q2 <- quantile(data_values, 0.5, na.rm = TRUE)  # Median
      q3 <- quantile(data_values, 0.75, na.rm = TRUE)

      # Add conditional formatting based on the calculated percentiles
      openxlsx::conditionalFormatting(wb, sheet = sheet_name, cols = (ncol(data) - n_cols + 1):ncol(data), rows = 2:(nrow(data) + 1),
                            rule = c(q1, q2, q3),
                            type = "colorScale",
                            style = c("#F8696B", "#FFEB84", "#63BE7B"))
    } else if (sheet_name == "house_weight") {
      # Calculate the 25th, 50th, and 75th percentiles for the entire data frame
      data_values <- unlist(data[, 2:ncol(data)], use.names = FALSE)
      q1 <- quantile(data_values, 0.25, na.rm = TRUE)
      q2 <- quantile(data_values, 0.5, na.rm = TRUE)  # Median
      q3 <- quantile(data_values, 0.75, na.rm = TRUE)

      # Add conditional formatting based on the calculated percentiles
      openxlsx::conditionalFormatting(wb, sheet = sheet_name, cols = 2:(ncol(data) + 1), rows = 2:(nrow(data) + 1),
                            rule = c(q1, q2, q3),
                            type = "colorScale",
                            style = c("#F8696B", "#FFEB84", "#63BE7B"))
    } else if (sheet_name %chin% c("visit_n", "feed_time", "feed_intake")) {
      # Calculate the 25th, 50th, and 75th percentiles for the entire data frame
      numeric_columns <- sapply(data, is.numeric)
      data_values <- unlist(data[, ..numeric_columns], use.names = FALSE)
      q1 <- quantile(data_values, 0.25, na.rm = TRUE)
      q2 <- quantile(data_values, 0.5, na.rm = TRUE)  # Median
      q3 <- quantile(data_values, 0.75, na.rm = TRUE)

      # Add conditional formatting based on the calculated percentiles
      openxlsx::conditionalFormatting(wb, sheet = sheet_name, cols = 2:(ncol(data) + 1), rows = 2:(nrow(data) + 1),
                            rule = c(q1, q2, q3),
                            type = "colorScale",
                            style = c("#F8696B", "#FFEB84", "#63BE7B"))
    } else {
      # Select columns with names containing "sum_n"
      sum_n_cols <- grep("sum_n", colnames(data), value = TRUE)
      n_cols <- length(sum_n_cols)

      # Calculate the 25th, 50th, and 75th percentiles for the entire data frame
      data_values <- unlist(data[, ..sum_n_cols], use.names = FALSE)
      q1 <- quantile(data_values, 0.25, na.rm = TRUE)
      q2 <- quantile(data_values, 0.5, na.rm = TRUE)  # Median
      q3 <- quantile(data_values, 0.75, na.rm = TRUE)

      # Add conditional formatting based on the calculated percentiles
      openxlsx::conditionalFormatting(wb, sheet = sheet_name, cols = (ncol(data) - n_cols + 1):ncol(data), rows = 2:(nrow(data) + 1),
                            rule = c(q1, q2, q3),
                            type = "colorScale",
                            style = c("#F8696B", "#FFEB84", "#63BE7B"))
    }

    # Create a bold style for column and row names
    header_style <- openxlsx::createStyle(textDecoration = "bold", halign = "center")

    # Apply the bold style to column names
    openxlsx::addStyle(wb, sheet = sheet_name, style = header_style, rows = 1, cols = 1:ncol(data), gridExpand = TRUE)

    # Create a center alignment style for cell contents
    center_style <- openxlsx::createStyle(halign = "center")

    # Apply the center alignment style to cell contents
    openxlsx::addStyle(wb, sheet = sheet_name, style = center_style, rows = 2:(nrow(data) + 1), cols = 1:ncol(data), gridExpand = TRUE)

    # Optionally, apply the bold style to row names
    # In this example, assume the first column contains row names
    openxlsx::addStyle(wb, sheet = sheet_name, style = header_style, rows = 1:(nrow(data) + 1), cols = 1, gridExpand = TRUE)

    # Auto adjust column widths
    openxlsx::setColWidths(wb, sheet = sheet_name, cols = 1:ncol(data), widths = "auto")
  }

  # Save the xlsx file
  openxlsx::saveWorkbook(wb, file.path(save_path, "other_monitor.xlsx"), overwrite = TRUE)

  return(all_monitor)
}

process_data <- function(data) {
  if (missing(data)) stop("Missing data frame or data table!")
  if (!is.data.frame(data) && !inherits(data, "data.table")) stop("Data is not a data frame or data table!")
  #if (!is.data.frame(data) && !class(data) == 'data.table')
  # Check for required columns
  required_columns <- c("animal_number", "lifenumber", "responder", "location", "visit_time", "duration", "state", "weight", "feed_intake")
  missing_columns <- setdiff(required_columns, names(data))
  if (length(missing_columns) > 0) stop(paste("Missing columns:", paste(missing_columns, collapse = ", ")))

  # Check types for some columns
  if (!is.numeric(data$animal_number) && !is.character(data$animal_number)) stop("'animal_number' must be numeric or character!")
  if (!is.logical(data$lifenumber) && !is.character(data$lifenumber)) stop("'lifenumber' must be logical or character!")
  if (!is.numeric(data$responder) && !is.character(data$responder)) stop("'responder' must be numeric or character!")
  if (!is.numeric(data$location)) stop("'location' must be numeric!")
  if (!is.character(data$visit_time) && !inherits(data$visit_time, "POSIXt")) stop("'visit_time' must be character or POSIXct!")
  if (!is.numeric(data$duration)) stop("'duration' must be numeric!")
  if (!is.numeric(data$state)) stop("'state' must be numeric!")
  if (!is.numeric(data$weight)) stop("'weight' must be numeric!")
  if (!is.numeric(data$feed_intake)) stop("'feed_intake' must be numeric!")

  # Check if the data is a data.frame, if yes, then make a deep copy of the data and convert it into data.table
  if (is.data.frame(data)) data <- data.table::as.data.table(data.table::copy(data))

  responder <- weight <- . <- location <- N <- n <- location_maxn <- visit_time <- seq_days <- seq_in_day <- seq_in_location <- feed_intake <- NULL

  # Filter out the data with NA in 'responder' column and remove duplicates
  data_temp <- unique(data)[!is.na(responder)]

  # Create a unique data.table for 'responder' and 'location'
  unique_dt <- unique(data_temp[, .(responder, location)])

  # Find duplicate 'responder's
  dup_responders <- unique_dt[, .N, by = .(responder)][N > 1]

  # Compute the number of records for each 'responder' and 'location'
  num_records <- unique(data_temp[, `:=`(n, .N), .(responder, location)][, .(responder, location, n)])

  # Set 'responder' as the key for join operations
  data.table::setkey(dup_responders, responder)
  data.table::setkey(num_records, responder)

  # Perform left join operation on 'num_records' and 'dup_responders'
  dup_records <- num_records[dup_responders]

  # Print duplicate 'responder's and their 'location's
  if(nrow(dup_records) > 0) {
    cat(crayon::red("\u2022 There are", length(unique(dup_records$responder)), "duplicated responders.\n"))
    print(dup_records)
  } else {
    cat(crayon::green("\u2022 There are no duplicate responders in different locations.\n"))
  }

  # Modify the 'location' in the unique data.table for duplicate 'responder's
  if(nrow(dup_responders) > 0) {
    # Compute the 'location' with maximum number of records for each 'responder'
    max_n_location <- num_records[, .(max_n = max(n), location_maxn = location[which.max(n)]), by = responder]

    # Remove duplicates in 'max_n_location' after modifying 'location'
    max_n_location <- unique(max_n_location)

    # Perform left join operation on 'data_temp' and 'max_n_location' and update 'location' to 'location_maxn'
    data_temp <- merge(data_temp, max_n_location, by = "responder", all.x = TRUE)[, location := location_maxn][, c("max_n", "location_maxn") := NULL]
  }

  # Preprocess data and compute sequence features
  # Check the class of visit_time
  if(is.character(data_temp$visit_time)) {
    # If visit_time is a character vector, replace "/" with "-"
    data_temp[, visit_time := gsub("/", "-", visit_time)]
    data_pre <- data_temp[, `:=`(c("date", "time"), data.table::IDateTime(visit_time))]
  } else {
    # If visit_time is not a character vector, assume it's a datetime object
    data_pre <- data_temp[, `:=`(c("date", "time"), data.table::IDateTime(visit_time))]
  }
  data_pre <- data_pre[data.table::CJ(date = tidyr::full_seq(date, 1)), on = .(date)  # Compute complete sequence of dates
  ][order(date), `:=`(seq_days, .GRP), by = date  # Compute sequence number of days
  ][order(visit_time), `:=`(seq_in_day, 1:.N), by = .(responder, date)  # Compute sequence number in day
  ][order(visit_time), `:=`(seq_in_location, 1:.N), by = .(location, date)  # Compute sequence number in location
  ][order(responder, visit_time)  # Order data by 'responder' and 'visit_time'
  ][, .(responder, location, date, feed_intake, weight)  # Keep only necessary columns #seq_in_location, seq_days, seq_in_day,, weight
  ][, `:=` (responder = as.character(responder),  # Convert 'responder' and 'location' to character type
            location = as.character(location),
            feed_intake = as.numeric(feed_intake),
            weight = as.numeric(weight))][!is.na(location)][]  # Convert 'weight' to numeric type
  return(data_pre)
}
low_feedintake <- function(data, days) {
  . <- feed_intake <- location <- responder <- sum_feedintake <- sum_feed_intake <- NULL
  # Filter rows, format the 'date', group by 'location', 'date', 'responder',
  # and calculate the sum of 'feed_intake' divided by 1000 where 'sum_feedintake' is less than 0.5
  processed_data <- data[date >= lubridate::today() - days][
    , date := format(date, "%y-%m-%d")][
      , .(sum_feedintake = sum(feed_intake) / 1000), by = .(location, date, responder)][
        sum_feedintake < 0.5
      ]

  # Convert to wide format with 'dcast'
  data_wide <- dcast(processed_data, location + responder ~ date, value.var = "sum_feedintake")

  # Sort by 'location' and calculate sum across numeric columns for each row
  numeric_cols <- setdiff(names(data_wide), c("location", "responder"))
  data_wide[, sum_feed_intake := round(rowSums(.SD, na.rm = TRUE), digits = 2), .SDcols = numeric_cols]
  #data_wide[, sum_feed_intake := round(Reduce(`+`, lapply(.SD, function(x) replace(x, is.na(x), 0))), 3), .SDcols = numeric_cols]

  # Re-sort based on 'sum_feed_intake'
  setorder(data_wide, sum_feed_intake)

  return(data_wide)
}
all_feedintake <- function(data, days) {
  . <- feed_intake <- location <- location_maxn <- sum_feedintake <- sum_feed_intake <- NULL
  # Step 1: Group by location and date, calculate the sum of feed intake divided by 1000
  # Filter the results for dates within the last week from November 1, 2023, format the date, and exclude rows with NA location
  res1 <- data[, .(sum_feedintake = sum(feed_intake) / 1000), by = .(location, date)
  ][date >= lubridate::today() - days
  ][, date := format(date, "%y-%m-%d")
  ]

  # Step 2: Transform the data into wide format using dcast
  res1_wide <- dcast(res1, location ~ date, value.var = "sum_feedintake")

  # Step 3: Identify numeric columns, excluding 'location' and potentially 'responder' columns
  numeric_cols <- setdiff(names(res1_wide), c("location", "responder"))

  # Step 4: Calculate the sum of all numeric columns for each row, ignoring NA values, and round the result to two decimal places
  res1_wide[, sum_feed_intake := round(rowSums(.SD, na.rm = TRUE), digits = 2), .SDcols = numeric_cols]

  # Return the processed data table
  return(res1_wide)
}
number_location <- function(data, house_width, days) {
  house <- location <- . <- responder <- NULL
  data[, house := substr(location, 1, house_width) # Step 1: Create 'house' column from the first character of 'location'
  ][date >= lubridate::today() - days # Step 2: Filter rows where 'date' is within the last 7 days from 2023-11-01
  ][, date := format(date, "%y-%m-%d") # Step 3: Format 'date' as 'year-month-day'
  ][, .(house, date, responder) # Step 4: Select the 'house', 'date', and 'responder' columns
  ][, unique(.SD, by = c("house", "date", "responder")) # Step 5: Remove duplicate rows based on 'house', 'date', 'responder'
  ][, .(n = .N), by = .(house, date) # Step 6: Group by 'house' and 'date', then count the number of rows in each group
  ][, dcast(.SD, date ~ house, value.var = "n")] # Step 7: Reshape the data into a wide format with 'date' as rows, 'house' as columns, and 'n' as values
}
house_feedintake <- function(data, house_width, days) {
  house <- location <- sum_feed_intake <- feed_intake <- . <- NULL
  temp <- data[, house := substr(location, 1, house_width)  # Step 1: Add a new column 'house' derived from the first character of 'location'.
  ][date >= lubridate::today() - days  # Step 2: Filter the data to include only the rows with 'date' within the last 7 days from 2023-11-01.
  ][, date := format(date, "%y-%m-%d")  # Step 3: Reformat 'date' to a 'year-month-day' string format.
  ][, sum_feed_intake := sum(feed_intake)/1000, by = .(house, date)  # Step 4: Sum 'feed_intake' divided by 1000 for each 'house' and 'date' group.
  ][, .(date, house, sum_feed_intake)  # Step 5: Select the 'date', 'house', and 'sum_feed_intake' columns for the output.
  ][, unique(.SD, by = c("house", "date"))  # Step 6: Remove duplicate rows based on 'house' and 'date'.
  ][, dcast(.SD, date ~ house, value.var = "sum_feed_intake")]  # Step 7: Reshape the data into a wide format with 'date' as rows, 'house' as columns, and summed feed intake as values.

  # After reshaping the data, calculate the total feed intake across all houses for each date.
  numeric_cols <- setdiff(names(temp), c("date"))  # Step 8: Identify numeric columns, excluding 'date'.
  temp[, sum_feed_intake := round(rowSums(.SD, na.rm = TRUE), digits = 2), .SDcols = numeric_cols]  # Step 9: Sum up the values across the numeric columns for each row, ignoring NAs, and round the result to two decimal places.

  return(temp)  # Step 10: Return the processed and reshaped data table.
}
merge_data <- function(feedintake, n) {
  . <- NULL
  # Merge hh2 and hh by "date", including all rows and adding suffixes to overlapping column names
  res3 <- merge(feedintake, n, by = "date", all = TRUE, suffixes = c("_feed", "_n"))

  # Extract column names exclusive to hh2 (excluding "date" and "sum_feed_intake")
  cols_feed <- names(feedintake)[!names(n) %in% c("date", "sum_feed_intake")]

  # Extract column names exclusive to hh (excluding "date")
  cols_n <- names(n)[!names(n) %in% c("date")]

  # Identify common columns between hh2 and hh for calculating mean feed intake
  intersect_house <- intersect(cols_feed, cols_n)

  # Identify columns unique to hh2 and hh, representing houses without corresponding data in the other table
  #set_diff_house_feed <- setdiff(cols_feed, cols_n) # Houses with feed data but no count data
  #set_diff_house_n <- setdiff(cols_n, cols_feed) # Houses with count data but no feed data

  # Append suffixes to column names for the merged table
  cols_feed_name <- paste0(intersect_house, "_feed")
  cols_n_name <- paste0(intersect_house, "_n")

  # Calculate mean feed intake for each common house and assign it to new columns
  res3[, paste0(intersect_house, "_mean_feed") := Map(function(feed, n) res3[[feed]] / res3[[n]],
                                                      cols_feed_name, cols_n_name)]

  # Step 3: Exclude non-numeric "_n" columns to focus on numeric analysis
  #numeric_cols <- setdiff(names(res3), grep("_n$", names(res3), value = TRUE))

  # Step 4: Isolate the data.table to numeric columns for further processing
  #res4 <- res3[, .(setdiff(names(res3), grep("_n$", names(res3), value = TRUE)))]

  # Identify and round double columns to three decimal places
  double_cols <- names(res3)[sapply(res3, is.double)]
  res3[, (double_cols) := lapply(.SD, function(x) round(x, digits = 3)), .SDcols = double_cols]

  return(res3)
}
house_weight <- function(data, house_width, days) {
  house <- location <- mean_weight <- weight <- . <- mean_weight_house <- NULL
  # Create a new column 'house' by concatenating "house_" with the first character of 'location'
  temp <- data[, house := paste0("house_", substr(location, 1, house_width))
               # Filter rows to include only those with 'date' within the last 7 days from 2023-11-01
  ][date >= lubridate::today() - days
    # Reformat the 'date' column to 'year-month-day'
  ][, date := format(date, "%y-%m-%d")
    # Calculate the mean 'weight' divided by 1000 for each 'location' and 'date', and create 'mean_weight' column
  ][, mean_weight := mean(weight)/1000, by = .(location, date)
    # Select specific columns 'date', 'house', 'location', and 'mean_weight' for the output
  ][, .(date, house, location, mean_weight)
    # Remove duplicate rows based on 'house', 'date', and 'location'
  ][, unique(.SD, by = c("house", "date", "location"))
    # Calculate the mean of 'mean_weight' for each 'house' and 'date', creating 'mean_weight_house'
  ][, mean_weight_house := mean(mean_weight), by = .(house, date)
    # Select specific columns 'date', 'house', and 'mean_weight_house' for the output
  ][, .(date, house, mean_weight_house)
    # Remove duplicate rows from the current selection
  ][, unique(.SD)
    # Reshape the data to a wide format, with 'date' as rows, 'house' as columns, and 'mean_weight_house' as values
  ][, dcast(.SD, date ~ house, value.var = "mean_weight_house")]

  # Identify numeric columns, excluding 'date'
  numeric_cols <- setdiff(names(temp), c("date"))
  # Round all numeric columns to two decimal places
  temp[, (numeric_cols) := lapply(.SD, function(x) round(x, digits = 2)), .SDcols = numeric_cols]
  # Return the processed data.table
  return(temp)
}
responder_na <- function(data, days) {
  visit_time <- responder <- n <- . <- location <- sum_na <- NULL
  # Convert 'visit_time' to IDate and ITime, then assign to 'date' and 'time' columns
  temp <- data[, `:=`(c("date", "time"), data.table::IDateTime(visit_time))
               # Filter rows to include only those with 'date' within the last 7 days
  ][date >= lubridate::today() - days
    # Remove duplicate rows based on all columns
  ][, unique(.SD)
    # Reformat 'date' column using 'visit_time' to 'year-month-day' format
  ][, date := format(visit_time, format="%y-%m-%d")
    # Filter rows where 'responder' is NA
  ][is.na(responder)
    # For each combination of 'location' and 'date', count the number of NAs in 'responder'
  ][, `:=`(n, .N), .(location, date)
    # Select 'location', 'date', and the count of NAs (n)
  ][, .(location, date, n)
    # Remove duplicate rows based on all columns again
  ][, unique(.SD)
    # Reshape the data to a wide format, with 'location' as rows, 'date' as columns, and 'n' as values
  ][, dcast(.SD, location ~ date, value.var = "n")
    # Ensure 'location' is treated as a character column
  ][, location := as.character(location)]

  # Identify numeric columns, excluding 'location'
  numeric_cols <- setdiff(names(temp), c("location"))
  # Sum up values across numeric columns for each row, ignoring NAs, and assign to 'sum_na'
  temp[, sum_na := rowSums(.SD, na.rm = TRUE), .SDcols = numeric_cols]

  # Return the processed data.table
  return(temp)
}
extreme_time_n <- function(data, days) {
  visit_time <- . <- location <- responder <- weight <- duration <- outlier <- is.extreme <- extreme_weight <- sum_time <- sum_n <- NULL
  # Split visit_time into date and time components
  data_temp <- data[, `:=`(c("date", "time"), data.table::IDateTime(visit_time))
                    # Filter records from the last 7 days
  ][date >= lubridate::today() - days
    # Remove duplicate rows
  ][, unique(.SD)
    # Reformat date in "yy-mm-dd" format
  ][, date := format(visit_time, format="%y-%m-%d")
    # Select only specific columns
  ][, .(location, date, responder, weight, duration)
    # Remove rows where location is NA
  ][!is.na(location)
  ]

  # Find outliers by location, date, and responder
  outlier_find <- data_temp[, .(location, date, responder, weight)
  ][, .(outlier = list(.SD)), by = .(location, date, responder)
    # Identify outliers using rstatix package
  ][, `:=`("outlier", purrr::map(.SD[[1]], \(x) rstatix::identify_outliers(x))), .SDcols = "outlier"
    # Bind rows of outliers into a single data.table
  ][, rbindlist(outlier), by = .(location, date, responder)]

  # Merge the original data with outlier findings
  data_merge <- merge(data_temp, outlier_find, all.x = TRUE)

  # Impute missing values for is.outlier and is.extreme as FALSE
  data_impute <- data_merge[, c("is.outlier", "is.extreme") :=
                              lapply(.SD, function(x) fifelse(is.na(x), FALSE, x)),
                            .SDcols = c("is.outlier", "is.extreme")]

  # Calculate the ratio of extreme values by location and date
  extreme_ratio <- data_impute[, .(extreme_weight = 100 * sum(is.extreme) / .N), by = .(location, date)
  ][, unique(.SD)
    # Filter for positive ratios
  ][extreme_weight > 0
    # Reshape data from long to wide format, setting True_Ratio by location and date
  ][, dcast(.SD, location~ date, value.var = "extreme_weight")]

  numeric_cols <- setdiff(names(extreme_ratio), c("location"))
  extreme_ratio[, (numeric_cols) := lapply(.SD, function(x) round(x, digits = 2)), .SDcols = numeric_cols]

  # Calculate total duration and count of visits by location and date
  feed_time_n <- data_impute[, sum_time := sum(duration)/3600, by = .(location, date)
  ][, `:=`(sum_n, .N), by = .(location, date)
  ][, .(location, date, sum_n, sum_time)
    # Remove duplicates
  ][, unique(.SD)
    # Reshape data from long to wide format, setting sum_time and sum_n by location and date
  ][, dcast(.SD, location~ date, value.var = c("sum_time", "sum_n"))]

  numeric_cols <- grep("time", names(feed_time_n), value = TRUE)
  feed_time_n[, (numeric_cols) := lapply(.SD, function(x) round(x, digits = 2)), .SDcols = numeric_cols]

  # Return a list of results
  return(list(extreme_ratio = extreme_ratio, feed_time_n = feed_time_n))
}
hour_stat <- function(data) {
  visit_time <- time <- location <- duration <- . <- n <- feedintake <- NULL
  # Split visit_time into date and time components
  data_temp <- data[, `:=`(c("date", "time"), data.table::IDateTime(visit_time))
                    # Filter records from the last 7 days
  ][date >= lubridate::today() - 1
    # Remove duplicate rows
  ][, unique(.SD)
    # Reformat date in "yy-mm-dd" format
  ][, date := format(visit_time, format="%y-%m-%d")
    # Select only specific columns
  ][, hour := as.integer(substr(time, 1, 2))][!is.na(location)
  ][, `:=`(n = .N, feed_time = sum(duration)/60, feedintake = sum(feed_intake)/1000), by = .(location, hour)
  ][, .(location, date, hour, n, feed_time, feedintake)
    # Remove rows where location is NA
  ][, unique(.SD)]

  numeric_cols <- grep("feed_time", names(data_temp), value = TRUE)
  data_temp <- data_temp[, (numeric_cols) := lapply(.SD, function(x) round(x, digits = 2)), .SDcols = numeric_cols]

  visit_n <- data_temp[, dcast(.SD, location ~ hour , value.var = "n")]
  feed_time <- data_temp[, dcast(.SD, location ~ hour , value.var = "feed_time")]
  feed_intake <- data_temp[, dcast(.SD, location ~ hour , value.var = "feedintake")]

  # Return a list of results
  return(list(visit_n = visit_n, feed_time = feed_time, feed_intake = feed_intake))
}
```

```{r examples-table_monitor}
# Load CSV data
data <- data.table::fread("C:/Users/Dell/Desktop/test/monitor_test_data.csv")
# Monitor station and data
res <- table_monitor(data = data, save_path = "C:/Users/Dell/Desktop/test")
# Monitor the number of times 'na' appears in the last 7 days
res$responder_na
# Monitor the percentage of extreme weight records in the last 7 days
res$extreme_weight
# Monitor the visiting time and frequency of pigs in the last 7 days
res$feed_time_n
# Monitor the total feed intake over the last 7 days
res$all_feedintake
# Monitor the average feed intake over the last 7 days
res$mean_feedintake
# Monitor the average weight per pen over the last 7 days
res$house_weight
# Monitor visit time in each hour over the last 1 day.
res$visit_n
# Monitor feed intake time in each hour over the last 1 day.
res$feed_time
# Monitor feed intake in each hour over the last 1 day.
res$feed_intake
```

# monitor_schedule

You can use `monitor_schedule` to set when to run the `fid_monitor()`、`station_monitor()` and `table_monitor()`.

```{r function-monitor_schedule}
#' Feed intake monitor of pig performance test station
#' 
#' @param taskname A character string with the name of the task. Defaults to the filename. Should not contain any spaces
#' @param schedule Either one of 'ONCE', 'MONTHLY', 'WEEKLY', 'DAILY', 'HOURLY', 'MINUTE', 'ONLOGON', 'ONIDLE
#' @param starttime A timepoint in HH:mm format indicating when to run the script. Defaults to within 62 seconds
#' @param startdate A date that specifies the first date on which to run the task. Only applicable if schedule is of type 'MONTHLY', 'WEEKLY', 'DAILY', 'HOURLY', 'MINUTE'. Defaults to today in '%d/%m/%Y' format. Change to your locale format if needed
#' @param rscript_args Character string with further arguments passed on to Rscript
#' @param ... other parameters
#'
#' @importFrom utils "capture.output"
#'
#' @return pdf and pngs
#' @export
#' 
monitor_schedule <- function(taskname, schedule, starttime, startdate, rscript_args = NULL, ...) {
  if (missing(taskname) || !is.character(taskname) || length(taskname) != 1) {
    stop("taskname must be a single character string")
  }

  if (missing(schedule) || !is.character(schedule) || length(schedule) != 1) {
    stop("schedule must be a single character string")
  }

  if (missing(starttime) || !is.character(starttime) || length(starttime) != 1) {
    stop("starttime must be a single character string")
  }

  if (missing(startdate) || !is.character(startdate) || length(startdate) != 1) {
    stop("startdate must be a single character string")
  }

  if (missing(rscript_args) || !is.list(rscript_args)) {
    stop("rscript_args must be a list of arguments")
  }

  # Save the function to a temporary script file with a shorter path
  short_temp_path <- "C:/Temp"
  dir.create(short_temp_path, showWarnings = FALSE)
  script_file <- file.path(short_temp_path, paste0(taskname, "_", sample(letters, 1), ".R"))

  my_function <- function(csv_path, begin_date, house_width, days, ...) {
    csv_files <- list.files(csv_path, full.names = T, pattern = ".csv", recursive = T)
    csv_data <- pptsda::import_csv(csv_files, package = "data.table")
    pptsdm::fid_monitor(data = csv_data, begin_date = begin_date, station_type = "nedap", ...)
    pptsdm::station_monitor(data = csv_data, begin_date = begin_date, station_type = "nedap", ...)
    pptsdm::table_monitor(data = csv_data, house_width = house_width, days = days,...)
  }

  # Save the arguments to a configuration file
  config_file <- file.path(short_temp_path, paste0("monitor_", taskname, ".txt"))
  cat("arg_list <- ", capture.output(dput(rscript_args)), file = config_file)

  write_function_to_script <- function(func, file_path, config_path) {
    func_name <- deparse(substitute(func))
    lines <- capture.output(dump(func_name, stdout()))
    lines <- c(lines, sprintf("source('%s')", config_path))
    lines <- c(lines, sprintf("do.call(%s, arg_list)", func_name))

    if (!file.exists(script_file)) {
      file.create(script_file)
    }

    con <- file(script_file, "w")
    on.exit(close(con), add = TRUE)
    writeLines(lines, con)
  }

  write_function_to_script(func = my_function, file_path = script_file, config_path = config_file)

  # Schedule the task
  taskscheduleR::taskscheduler_create(taskname = taskname,
                                      rscript = script_file,
                                      schedule = schedule,
                                      starttime = starttime,
                                      startdate = startdate,
                                      rscript_args = NULL,
                                      ...)
}
```

```{r examples-monitor_schedule}
# Set monitor task
monitor_schedule(
  taskname = "ppt_csv_monitor",
  schedule = "DAILY",
  starttime = "10:05",
  startdate = format(Sys.Date(), "%Y/%m/%d"),
  rscript_args = list(house_width = "1", 
                      days = 7,
                      begin_date = "2024-05-01", 
                      csv_path = "C:/Users/Dell/Desktop/test/NonEmptyCSVs",
                      save_path = "C:/Users/Dell/Desktop/test"))
# Delete monitor task
taskscheduleR::taskscheduler_delete("ppt_csv_monitor")
```

That's it ! This the end of the documented story of our package. All components are there.

```{=html}
<!-- 
# Inflate your package

You're one inflate from paper to box.
Build your package from this very Rmd using `fusen::inflate()` 
-->
```
```{r development-inflate, eval=FALSE}
# Execute in the console directly
fusen::inflate(flat_file = "dev/station_pig_monitor.Rmd", check = T, vignette_name = "Basic Usage")
```

```{=html}
<!-- 
- Verify your `"DESCRIPTION"` file has been updated
- Verify your function is in `"R/"` directory
- Verify your test is in `"tests/testthat/"` directory
- Verify this Rmd appears in `"vignettes/"` directory 
-->
```
